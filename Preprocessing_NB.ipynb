{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bada8c77",
   "metadata": {},
   "source": [
    "# **Avalanche Risk Project**\n",
    "\n",
    "Advanced Data Analytics, Fall 2025\n",
    "\n",
    "The following project examines the feasibility of machine learning models to predict dry avalanche danger from spatial and meteorological features for the Engelberg Valley. In a second step, the model will be trained on the whole of Switzerland and tested as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bbc72",
   "metadata": {},
   "source": [
    "### **1. Data Collection**\n",
    "\n",
    "I start by importing all necessary data from the different APIs including:\n",
    "\n",
    "- SLF Bulletin Archive\n",
    "- Meteo Swiss IMIS Data Archive\n",
    "- SwissTopo Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493da979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Libraries \n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6434d",
   "metadata": {},
   "source": [
    "Apparently, the API only returns data from 2024 onwards. That means I have to scrape the data from the SLF archive using `Beautiful Soup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6727eac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datum</th>\n",
       "      <th>station_code</th>\n",
       "      <th>sector_id</th>\n",
       "      <th>warnreg</th>\n",
       "      <th>elevation_station</th>\n",
       "      <th>forecast_initial_date</th>\n",
       "      <th>forecast_end_date</th>\n",
       "      <th>dangerLevel</th>\n",
       "      <th>elevation_th</th>\n",
       "      <th>...</th>\n",
       "      <th>ssi_pwl</th>\n",
       "      <th>sk38_pwl</th>\n",
       "      <th>sn38_pwl</th>\n",
       "      <th>ccl_pwl</th>\n",
       "      <th>ssi_pwl_100</th>\n",
       "      <th>sk38_pwl_100</th>\n",
       "      <th>sn38_pwl_100</th>\n",
       "      <th>ccl_pwl_100</th>\n",
       "      <th>Pen_depth</th>\n",
       "      <th>min_ccl_pen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1997-11-11</td>\n",
       "      <td>KES2</td>\n",
       "      <td>7113.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>1997-11-11 17:00:00</td>\n",
       "      <td>1997-11-12 17:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.30</td>\n",
       "      <td>44.028391</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1997-11-11</td>\n",
       "      <td>SIM2</td>\n",
       "      <td>6113.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1997-11-11 17:00:00</td>\n",
       "      <td>1997-11-12 17:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>37.271809</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-11</td>\n",
       "      <td>DTR2</td>\n",
       "      <td>6113.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1997-11-11 17:00:00</td>\n",
       "      <td>1997-11-12 17:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.12</td>\n",
       "      <td>38.369101</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1997-11-11</td>\n",
       "      <td>MEI2</td>\n",
       "      <td>2221.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1997-11-11 17:00:00</td>\n",
       "      <td>1997-11-12 17:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1997-11-11</td>\n",
       "      <td>SPN2</td>\n",
       "      <td>4232.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>1997-11-11 17:00:00</td>\n",
       "      <td>1997-11-12 17:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>42.332551</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292832</th>\n",
       "      <td>292832</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>FIR2</td>\n",
       "      <td>1242.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2020-05-04 17:00:00</td>\n",
       "      <td>2020-05-05 17:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.299643</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292833</th>\n",
       "      <td>292833</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>GRA2</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2020-05-04 17:00:00</td>\n",
       "      <td>2020-05-05 17:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.881834</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292834</th>\n",
       "      <td>292834</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>SHE2</td>\n",
       "      <td>1213.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2020-05-04 17:00:00</td>\n",
       "      <td>2020-05-05 17:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292835</th>\n",
       "      <td>292835</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>ELS2</td>\n",
       "      <td>1231.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2020-05-04 17:00:00</td>\n",
       "      <td>2020-05-05 17:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292836</th>\n",
       "      <td>292836</td>\n",
       "      <td>2020-05-04</td>\n",
       "      <td>LAU2</td>\n",
       "      <td>1222.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2020-05-04 17:00:00</td>\n",
       "      <td>2020-05-05 17:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292837 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       datum station_code  sector_id  warnreg  \\\n",
       "0                0  1997-11-11         KES2     7113.0     15.0   \n",
       "1                1  1997-11-11         SIM2     6113.0     15.0   \n",
       "2                2  1997-11-11         DTR2     6113.0     15.0   \n",
       "3                3  1997-11-11         MEI2     2221.0     15.0   \n",
       "4                4  1997-11-11         SPN2     4232.0     15.0   \n",
       "...            ...         ...          ...        ...      ...   \n",
       "292832      292832  2020-05-04         FIR2     1242.0     21.0   \n",
       "292833      292833  2020-05-04         GRA2     1311.0     21.0   \n",
       "292834      292834  2020-05-04         SHE2     1213.0     21.0   \n",
       "292835      292835  2020-05-04         ELS2     1231.0     21.0   \n",
       "292836      292836  2020-05-04         LAU2     1222.0     21.0   \n",
       "\n",
       "        elevation_station forecast_initial_date    forecast_end_date  \\\n",
       "0                  2700.0   1997-11-11 17:00:00  1997-11-12 17:00:00   \n",
       "1                  2400.0   1997-11-11 17:00:00  1997-11-12 17:00:00   \n",
       "2                  2100.0   1997-11-11 17:00:00  1997-11-12 17:00:00   \n",
       "3                  2200.0   1997-11-11 17:00:00  1997-11-12 17:00:00   \n",
       "4                  2600.0   1997-11-11 17:00:00  1997-11-12 17:00:00   \n",
       "...                   ...                   ...                  ...   \n",
       "292832             2100.0   2020-05-04 17:00:00  2020-05-05 17:00:00   \n",
       "292833             2000.0   2020-05-04 17:00:00  2020-05-05 17:00:00   \n",
       "292834             1900.0   2020-05-04 17:00:00  2020-05-05 17:00:00   \n",
       "292835             2100.0   2020-05-04 17:00:00  2020-05-05 17:00:00   \n",
       "292836             2000.0   2020-05-04 17:00:00  2020-05-05 17:00:00   \n",
       "\n",
       "        dangerLevel  elevation_th  ... ssi_pwl  sk38_pwl  sn38_pwl  ccl_pwl  \\\n",
       "0               1.0        2000.0  ...    2.02      1.02      1.92     0.30   \n",
       "1               2.0        2000.0  ...    6.00      6.00      6.00     4.00   \n",
       "2               2.0        2000.0  ...    1.44      0.44      1.37     0.12   \n",
       "3               1.0        2000.0  ...    6.00      6.00      6.00     0.20   \n",
       "4               2.0        2000.0  ...    6.00      6.00      6.00     4.00   \n",
       "...             ...           ...  ...     ...       ...       ...      ...   \n",
       "292832          2.0        1800.0  ...    6.00      6.00      6.00     4.00   \n",
       "292833          2.0        1800.0  ...    6.00      6.00      6.00     4.00   \n",
       "292834          2.0        1800.0  ...     NaN       NaN       NaN      NaN   \n",
       "292835          2.0        1800.0  ...     NaN       NaN       NaN      NaN   \n",
       "292836          2.0        1800.0  ...     NaN       NaN       NaN      NaN   \n",
       "\n",
       "        ssi_pwl_100  sk38_pwl_100  sn38_pwl_100  ccl_pwl_100  Pen_depth  \\\n",
       "0              2.02          1.02          1.92         0.30  44.028391   \n",
       "1              6.00          6.00          6.00         4.00  37.271809   \n",
       "2              1.44          0.44          1.37         0.12  38.369101   \n",
       "3              6.00          6.00          6.00         0.20  20.400000   \n",
       "4              6.00          6.00          6.00         4.00  42.332551   \n",
       "...             ...           ...           ...          ...        ...   \n",
       "292832         6.00          6.00          6.00         4.00   6.299643   \n",
       "292833         6.00          6.00          6.00         4.00   6.881834   \n",
       "292834          NaN           NaN           NaN          NaN        NaN   \n",
       "292835          NaN           NaN           NaN          NaN        NaN   \n",
       "292836          NaN           NaN           NaN          NaN        NaN   \n",
       "\n",
       "        min_ccl_pen  \n",
       "0              0.17  \n",
       "1              0.26  \n",
       "2              0.12  \n",
       "3              4.00  \n",
       "4              0.16  \n",
       "...             ...  \n",
       "292832         3.00  \n",
       "292833         3.00  \n",
       "292834          NaN  \n",
       "292835          NaN  \n",
       "292836          NaN  \n",
       "\n",
       "[292837 rows x 78 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/users/fabio/Downloads/data_rf1_forecast.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a40217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'datum', 'station_code', 'sector_id', 'warnreg',\n",
      "       'elevation_station', 'forecast_initial_date', 'forecast_end_date',\n",
      "       'dangerLevel', 'elevation_th', 'set', 'Qs', 'Ql', 'TSG', 'Qg0', 'Qr',\n",
      "       'OLWR', 'ILWR', 'LWR_net', 'OSWR', 'ISWR', 'Qw', 'pAlbedo', 'ISWR_h',\n",
      "       'ISWR_diff', 'ISWR_dir', 'TA', 'TSS_mod', 'TSS_meas', 'T_bottom', 'RH',\n",
      "       'VW', 'VW_drift', 'DW', 'MS_Snow', 'HS_mod', 'HS_meas', 'hoar_size',\n",
      "       'wind_trans24', 'wind_trans24_7d', 'wind_trans24_3d', 'HN24', 'HN72_24',\n",
      "       'HN24_7d', 'SWE', 'MS_water', 'MS_Wind', 'MS_Rain', 'MS_SN_Runoff',\n",
      "       'MS_Sublimation', 'MS_Evap', 'TS0', 'TS1', 'TS2', 'Sclass2', 'zSd_mean',\n",
      "       'Sd', 'zSn', 'Sn', 'zSs', 'Ss', 'zS4', 'S4', 'zS5', 'S5', 'pwl_100',\n",
      "       'pwl_100_15', 'base_pwl', 'ssi_pwl', 'sk38_pwl', 'sn38_pwl', 'ccl_pwl',\n",
      "       'ssi_pwl_100', 'sk38_pwl_100', 'sn38_pwl_100', 'ccl_pwl_100',\n",
      "       'Pen_depth', 'min_ccl_pen'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2c27a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching list of available stations...\n",
      "Found 249 station files\n",
      "\n",
      "Fetching data for 249 stations...\n",
      "This may take several minutes...\n",
      "\n",
      "[1/249] Fetching ADE2.csv... (49714 rows)\n",
      "[2/249] Fetching ADE3.csv... (86572 rows)\n",
      "[3/249] Fetching ALB2.csv... (105770 rows)\n",
      "[4/249] Fetching ALI1.csv... (366371 rows)\n",
      "[5/249] Fetching ALI2.csv... (366255 rows)\n",
      "[6/249] Fetching AMD1.csv... (488963 rows)\n",
      "[7/249] Fetching AMD2.csv... (474407 rows)\n",
      "[8/249] Fetching ANV1.csv... (490298 rows)\n",
      "[9/249] Fetching ANV2.csv... (488028 rows)\n",
      "[10/249] Fetching ANV3.csv... (487514 rows)\n",
      "[11/249] Fetching ARO1.csv... (478877 rows)\n",
      "[12/249] Fetching ARO2.csv... (470824 rows)\n",
      "[13/249] Fetching ARO3.csv... (481778 rows)\n",
      "[14/249] Fetching ATT2.csv... (397188 rows)\n",
      "[15/249] Fetching AUE2.csv... (12259 rows)\n",
      "[16/249] Fetching BAG1.csv... (50063 rows)\n",
      "[17/249] Fetching BAR2.csv... (9955 rows)\n",
      "[18/249] Fetching BED1.csv... (501100 rows)\n",
      "[19/249] Fetching BED2.csv... (425053 rows)\n",
      "[20/249] Fetching BED3.csv... (490576 rows)\n",
      "[21/249] Fetching BED4.csv... (71696 rows)\n",
      "[22/249] Fetching BEL2.csv... (292897 rows)\n",
      "[23/249] Fetching BER1.csv... (430912 rows)\n",
      "[24/249] Fetching BER2.csv... (428829 rows)\n",
      "[25/249] Fetching BER3.csv... (432010 rows)\n",
      "[26/249] Fetching BEV1.csv... (490168 rows)\n",
      "[27/249] Fetching BEV2.csv... (489782 rows)\n",
      "[28/249] Fetching BIN2.csv... (34633 rows)\n",
      "[29/249] Fetching BIN3.csv... (17802 rows)\n",
      "[30/249] Fetching BIN4.csv... (70684 rows)\n",
      "[31/249] Fetching BOG1.csv... (451103 rows)\n",
      "[32/249] Fetching BOG2.csv... (449227 rows)\n",
      "[33/249] Fetching BOG3.csv... (309148 rows)\n",
      "[34/249] Fetching BOG4.csv... (309016 rows)\n",
      "[35/249] Fetching BOR2.csv... (419841 rows)\n",
      "[36/249] Fetching BOV1.csv... (413968 rows)\n",
      "[37/249] Fetching BOV2.csv... (417131 rows)\n",
      "[38/249] Fetching CAM1.csv... (446527 rows)\n",
      "[39/249] Fetching CAM2.csv... (448890 rows)\n",
      "[40/249] Fetching CAM3.csv... (188716 rows)\n",
      "[41/249] Fetching CHA1.csv... (477915 rows)\n",
      "[42/249] Fetching CHA2.csv... (479457 rows)\n",
      "[43/249] Fetching CHA3.csv... (15632 rows)\n",
      "[44/249] Fetching CMA2.csv... (411141 rows)\n",
      "[45/249] Fetching CON2.csv... (401018 rows)\n",
      "[46/249] Fetching COR2.csv... (47029 rows)\n",
      "[47/249] Fetching CSL2.csv... (133949 rows)\n",
      "[48/249] Fetching DAV1.csv... (470737 rows)\n",
      "[49/249] Fetching DAV2.csv... (471528 rows)\n",
      "[50/249] Fetching DAV3.csv... (471440 rows)\n",
      "[51/249] Fetching DAV4.csv... (408438 rows)\n",
      "[52/249] Fetching DAV5.csv... (345227 rows)\n",
      "[53/249] Fetching DAV6.csv... (51838 rows)\n",
      "[54/249] Fetching DIA2.csv... (375701 rows)\n",
      "[55/249] Fetching DTR1.csv... (489748 rows)\n",
      "[56/249] Fetching DTR2.csv... (489932 rows)\n",
      "[57/249] Fetching EGH2.csv... (389598 rows)\n",
      "[58/249] Fetching ELA1.csv... (299138 rows)\n",
      "[59/249] Fetching ELA2.csv... (299404 rows)\n",
      "[60/249] Fetching ELA3.csv... (298681 rows)\n",
      "[61/249] Fetching ELM1.csv... (436000 rows)\n",
      "[62/249] Fetching ELM2.csv... (436136 rows)\n",
      "[63/249] Fetching ELM3.csv... (17699 rows)\n",
      "[64/249] Fetching ELM4.csv... (17699 rows)\n",
      "[65/249] Fetching ELS1.csv... (484480 rows)\n",
      "[66/249] Fetching ELS2.csv... (470784 rows)\n",
      "[67/249] Fetching ENG1.csv... (104284 rows)\n",
      "[68/249] Fetching ENG2.csv... (104287 rows)\n",
      "[69/249] Fetching ERN1.csv... (42240 rows)\n",
      "[70/249] Fetching ERN2.csv... (46694 rows)\n",
      "[71/249] Fetching EVO2.csv... (132080 rows)\n",
      "[72/249] Fetching EVO3.csv... (132066 rows)\n",
      "[73/249] Fetching FAE1.csv... (480184 rows)\n",
      "[74/249] Fetching FAE2.csv... (484411 rows)\n",
      "[75/249] Fetching FIR2.csv... (472413 rows)\n",
      "[76/249] Fetching FIS2.csv... (483844 rows)\n",
      "[77/249] Fetching FLU2.csv... (381029 rows)\n",
      "[78/249] Fetching FNH1.csv... (479779 rows)\n",
      "[79/249] Fetching FNH2.csv... (479648 rows)\n",
      "[80/249] Fetching FOU1.csv... (451479 rows)\n",
      "[81/249] Fetching FOU2.csv... (158454 rows)\n",
      "[82/249] Fetching FOU3.csv... (89837 rows)\n",
      "[83/249] Fetching FOU4.csv... (155547 rows)\n",
      "[84/249] Fetching FRA2.csv... (367136 rows)\n",
      "[85/249] Fetching FRA3.csv... (365152 rows)\n",
      "[86/249] Fetching FUL1.csv... Error fetching FUL1.csv: ('Connection broken: IncompleteRead(6985938 bytes read, 21352622 more expected)', IncompleteRead(6985938 bytes read, 21352622 more expected))\n",
      "Failed\n",
      "[87/249] Fetching FUL2.csv... (450962 rows)\n",
      "[88/249] Fetching FUS2.csv... (412390 rows)\n",
      "[89/249] Fetching GAD2.csv... (468664 rows)\n",
      "[90/249] Fetching GAL1.csv... (47535 rows)\n",
      "[91/249] Fetching GAL2.csv... (44441 rows)\n",
      "[92/249] Fetching GAN1.csv... (503133 rows)\n",
      "[93/249] Fetching GAN2.csv... (501502 rows)\n",
      "[94/249] Fetching GLA1.csv... (381040 rows)\n",
      "[95/249] Fetching GLA2.csv... (435961 rows)\n",
      "[96/249] Fetching GOM1.csv... (448428 rows)\n",
      "[97/249] Fetching GOM2.csv... (452802 rows)\n",
      "[98/249] Fetching GOM3.csv... (454650 rows)\n",
      "[99/249] Fetching GOR2.csv... (399605 rows)\n",
      "[100/249] Fetching GOS2.csv... (70694 rows)\n",
      "[101/249] Fetching GOS3.csv... (70683 rows)\n",
      "[102/249] Fetching GRA1.csv... (349043 rows)\n",
      "[103/249] Fetching GRA2.csv... (349023 rows)\n",
      "[104/249] Fetching GUG2.csv... (53696 rows)\n",
      "[105/249] Fetching GUT1.csv... (469943 rows)\n",
      "[106/249] Fetching GUT2.csv... (453567 rows)\n",
      "[107/249] Fetching GUT3.csv... (68458 rows)\n",
      "[108/249] Fetching HIN1.csv... (438182 rows)\n",
      "[109/249] Fetching HTR1.csv... (417912 rows)\n",
      "[110/249] Fetching HTR2.csv... (417184 rows)\n",
      "[111/249] Fetching HTR3.csv... (418639 rows)\n",
      "[112/249] Fetching ILI1.csv... (433952 rows)\n",
      "[113/249] Fetching ILI2.csv... (437206 rows)\n",
      "[114/249] Fetching JAU1.csv... (366310 rows)\n",
      "[115/249] Fetching JAU2.csv... (366143 rows)\n",
      "[116/249] Fetching JUL1.csv... (462204 rows)\n",
      "[117/249] Fetching JUL2.csv... (461076 rows)\n",
      "[118/249] Fetching KES1.csv... (482432 rows)\n",
      "[119/249] Fetching KES2.csv... (488621 rows)\n",
      "[120/249] Fetching KLO1.csv... (504576 rows)\n",
      "[121/249] Fetching KLO2.csv... (506275 rows)\n",
      "[122/249] Fetching KLO3.csv... (500827 rows)\n",
      "[123/249] Fetching KLT1.csv... (50244 rows)\n",
      "[124/249] Fetching KLT2.csv... (30397 rows)\n",
      "[125/249] Fetching KLT3.csv... (139788 rows)\n",
      "[126/249] Fetching KLT4.csv... (41486 rows)\n",
      "[127/249] Fetching LAG1.csv... (494329 rows)\n",
      "[128/249] Fetching LAG2.csv... (493963 rows)\n",
      "[129/249] Fetching LAG3.csv... (274649 rows)\n",
      "[130/249] Fetching LAG4.csv... (217965 rows)\n",
      "[131/249] Fetching LAU1.csv... (451486 rows)\n",
      "[132/249] Fetching LAU2.csv... (452258 rows)\n",
      "[133/249] Fetching LAV1.csv... (277947 rows)\n",
      "[134/249] Fetching LHO2.csv... (418865 rows)\n",
      "[135/249] Fetching LHO3.csv... (186466 rows)\n",
      "[136/249] Fetching LUK1.csv... (430475 rows)\n",
      "[137/249] Fetching LUK2.csv... (437061 rows)\n",
      "[138/249] Fetching LUM2.csv... (478727 rows)\n",
      "[139/249] Fetching LUM3.csv... (478587 rows)\n",
      "[140/249] Fetching MAE2.csv... (379198 rows)\n",
      "[141/249] Fetching MAN1.csv... (189947 rows)\n",
      "[142/249] Fetching MEI2.csv... (491086 rows)\n",
      "[143/249] Fetching MES1.csv... (402226 rows)\n",
      "[144/249] Fetching MES2.csv... (402096 rows)\n",
      "[145/249] Fetching MLB1.csv... (298803 rows)\n",
      "[146/249] Fetching MLB2.csv... (298684 rows)\n",
      "[147/249] Fetching MTR2.csv... (378787 rows)\n",
      "[148/249] Fetching MUN1.csv... (434985 rows)\n",
      "[149/249] Fetching MUN2.csv... (434499 rows)\n",
      "[150/249] Fetching MUO1.csv... (331954 rows)\n",
      "[151/249] Fetching MUO2.csv... (332006 rows)\n",
      "[152/249] Fetching MUT1.csv... (298448 rows)\n",
      "[153/249] Fetching MUT2.csv... (298418 rows)\n",
      "[154/249] Fetching NAR1.csv... (505088 rows)\n",
      "[155/249] Fetching NAR2.csv... (505874 rows)\n",
      "[156/249] Fetching NAS2.csv... (415375 rows)\n",
      "[157/249] Fetching NEN1.csv... (442740 rows)\n",
      "[158/249] Fetching NEN2.csv... (241146 rows)\n",
      "[159/249] Fetching NEN3.csv... (240833 rows)\n",
      "[160/249] Fetching OBM2.csv... (432805 rows)\n",
      "[161/249] Fetching OBW1.csv... (452674 rows)\n",
      "[162/249] Fetching OBW2.csv... (454719 rows)\n",
      "[163/249] Fetching OBW3.csv... (445523 rows)\n",
      "[164/249] Fetching OFE1.csv... (400223 rows)\n",
      "[165/249] Fetching OFE2.csv... (403729 rows)\n",
      "[166/249] Fetching ORT2.csv... (434181 rows)\n",
      "[167/249] Fetching OTT2.csv... (477192 rows)\n",
      "[168/249] Fetching OUJ1.csv... (74169 rows)\n",
      "[169/249] Fetching OUJ2.csv... (74534 rows)\n",
      "[170/249] Fetching PAR2.csv... (488079 rows)\n",
      "[171/249] Fetching PLD2.csv... (175824 rows)\n",
      "[172/249] Fetching PMA2.csv... (399739 rows)\n",
      "[173/249] Fetching PON2.csv... (12890 rows)\n",
      "[174/249] Fetching PUZ1.csv... (503526 rows)\n",
      "[175/249] Fetching PUZ2.csv... (505927 rows)\n",
      "[176/249] Fetching REV2.csv... (12483 rows)\n",
      "[177/249] Fetching RGS2.csv... (16937 rows)\n",
      "[178/249] Fetching RNZ2.csv... (273545 rows)\n",
      "[179/249] Fetching RNZ3.csv... (16450 rows)\n",
      "[180/249] Fetching ROA1.csv... (506606 rows)\n",
      "[181/249] Fetching ROA2.csv... (506327 rows)\n",
      "[182/249] Fetching ROA4.csv... (156602 rows)\n",
      "[183/249] Fetching ROT2.csv... (224693 rows)\n",
      "[184/249] Fetching ROT3.csv... (122342 rows)\n",
      "[185/249] Fetching SAA1.csv... (489878 rows)\n",
      "[186/249] Fetching SAA2.csv... (489536 rows)\n",
      "[187/249] Fetching SAA3.csv... (351018 rows)\n",
      "[188/249] Fetching SAA4.csv... (274376 rows)\n",
      "[189/249] Fetching SAA5.csv... (138849 rows)\n",
      "[190/249] Fetching SCA1.csv... (455934 rows)\n",
      "[191/249] Fetching SCA2.csv... (458709 rows)\n",
      "[192/249] Fetching SCA3.csv... (458222 rows)\n",
      "[193/249] Fetching SCB2.csv... (435244 rows)\n",
      "[194/249] Fetching SCH1.csv... (505672 rows)\n",
      "[195/249] Fetching SCH2.csv... (504029 rows)\n",
      "[196/249] Fetching SHE2.csv... (419786 rows)\n",
      "[197/249] Fetching SIM1.csv... (506409 rows)\n",
      "[198/249] Fetching SIM2.csv... (507849 rows)\n",
      "[199/249] Fetching SLF2.csv... (462053 rows)\n",
      "[200/249] Fetching SMN1.csv... (15844 rows)\n",
      "[201/249] Fetching SMN2.csv... (435641 rows)\n",
      "[202/249] Fetching SPN1.csv... (494701 rows)\n",
      "[203/249] Fetching SPN2.csv... (491816 rows)\n",
      "[204/249] Fetching SPN3.csv... (475569 rows)\n",
      "[205/249] Fetching STE2.csv... (22860 rows)\n",
      "[206/249] Fetching STH1.csv... (417104 rows)\n",
      "[207/249] Fetching STH2.csv... (423371 rows)\n",
      "[208/249] Fetching STN2.csv... (460288 rows)\n",
      "[209/249] Fetching SWM1.csv... (279608 rows)\n",
      "[210/249] Fetching SWM2.csv... (18028 rows)\n",
      "[211/249] Fetching TAM1.csv... (433678 rows)\n",
      "[212/249] Fetching TAM2.csv... (432827 rows)\n",
      "[213/249] Fetching TAM3.csv... (432127 rows)\n",
      "[214/249] Fetching TGL2.csv... (13425 rows)\n",
      "[215/249] Fetching TIT2.csv... (400726 rows)\n",
      "[216/249] Fetching TRU1.csv... (504117 rows)\n",
      "[217/249] Fetching TRU2.csv... (501712 rows)\n",
      "[218/249] Fetching TUJ1.csv... (489467 rows)\n",
      "[219/249] Fetching TUJ2.csv... (488471 rows)\n",
      "[220/249] Fetching TUJ3.csv... (315946 rows)\n",
      "[221/249] Fetching TUJ4.csv... (174581 rows)\n",
      "[222/249] Fetching TUM1.csv... (402263 rows)\n",
      "[223/249] Fetching TUM2.csv... (402249 rows)\n",
      "[224/249] Fetching URS1.csv... (491479 rows)\n",
      "[225/249] Fetching URS2.csv... (491258 rows)\n",
      "[226/249] Fetching VAL1.csv... (500859 rows)\n",
      "[227/249] Fetching VAL2.csv... (506510 rows)\n",
      "[228/249] Fetching VDS1.csv... (464248 rows)\n",
      "[229/249] Fetching VDS2.csv... (470419 rows)\n",
      "[230/249] Fetching VIA2.csv... (19957 rows)\n",
      "[231/249] Fetching VIN1.csv... (472681 rows)\n",
      "[232/249] Fetching VIN2.csv... (473209 rows)\n",
      "[233/249] Fetching VLS1.csv... (419368 rows)\n",
      "[234/249] Fetching VLS2.csv... (422237 rows)\n",
      "[235/249] Fetching VMA2.csv... (49215 rows)\n",
      "[236/249] Fetching VRG2.csv... (13371 rows)\n",
      "[237/249] Fetching VRG3.csv... (52731 rows)\n",
      "[238/249] Fetching VSC1.csv... (88583 rows)\n",
      "[239/249] Fetching VSC2.csv... (88591 rows)\n",
      "[240/249] Fetching WAS2.csv... (13882 rows)\n",
      "[241/249] Fetching WFJ2.csv... (590676 rows)\n",
      "[242/249] Fetching YBR2.csv... (331659 rows)\n",
      "[243/249] Fetching ZER1.csv... (470278 rows)\n",
      "[244/249] Fetching ZER2.csv... (472017 rows)\n",
      "[245/249] Fetching ZER3.csv... (70869 rows)\n",
      "[246/249] Fetching ZER4.csv... (315472 rows)\n",
      "[247/249] Fetching ZER5.csv... (51057 rows)\n",
      "[248/249] Fetching ZNZ1.csv... (404293 rows)\n",
      "[249/249] Fetching ZNZ2.csv... (404219 rows)\n",
      "\n",
      "============================================================\n",
      "Data collection complete!\n",
      "Successful: 248/249\n",
      "Failed: 1/249\n",
      "============================================================\n",
      "\n",
      "Combining all station data...\n",
      "Combined dataset shape: (84404943, 19)\n",
      "Columns: ['station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN,DW_30MIN_SD,HS,TS0_30MIN_MEAN,TS25_30MIN_MEAN,TS50_30MIN_MEAN,TS100_30MIN_MEAN,RSWR_30MIN_MEAN,TSS_30MIN_MEAN', 'station_code', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN,DW_30MIN_SD', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN,DW_30MIN_SD,HS,TS0_30MIN_MEAN,RSWR_30MIN_MEAN,TSS_30MIN_MEAN', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN,HS,TS0_30MIN_MEAN,TS25_30MIN_MEAN,TS50_30MIN_MEAN,TS100_30MIN_MEAN,RSWR_30MIN_MEAN,TSS_30MIN_MEAN', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN,DW_30MIN_SD,HS', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN,HS', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN,DW_30MIN_SD,HS,RSWR_30MIN_MEAN,TSS_30MIN_MEAN', 'station_code,measure_date,hyear,TA_30MIN_MEAN,RH_30MIN_MEAN,HS,TS0_30MIN_MEAN', 'station_code,measure_date,hyear,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN', 'station_code,measure_date,hyear,TA_30MIN_MEAN,HS,TS25_30MIN_MEAN,TS50_30MIN_MEAN,TS100_30MIN_MEAN', 'station_code,measure_date,hyear,VW_30MIN_MEAN,VW_30MIN_MAX', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN', 'station_code,measure_date,hyear,TA_30MIN_MEAN,RH_30MIN_MEAN,HS,TS0_30MIN_MEAN,TS25_30MIN_MEAN,TS50_30MIN_MEAN,TS100_30MIN_MEAN,RSWR_30MIN_MEAN,TSS_30MIN_MEAN', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,DW_30MIN_SD', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN,DW_30MIN_SD,HS,TS0_30MIN_MEAN,TS25_30MIN_MEAN,RSWR_30MIN_MEAN,TSS_30MIN_MEAN', 'station_code,measure_date,hyear,TA_30MIN_MEAN,RH_30MIN_MEAN,DW_30MIN_SD,HS,TS0_30MIN_MEAN,TS25_30MIN_MEAN,TS50_30MIN_MEAN,RSWR_30MIN_MEAN,TSS_30MIN_MEAN', 'station_code,measure_date,hyear,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,DW_30MIN_SD', 'station_code,measure_date,hyear,TA_30MIN_MEAN,VW_30MIN_MEAN,VW_30MIN_MAX,DW_30MIN_MEAN,RH_30MIN_MEAN,HS,TS0_30MIN_MEAN,TS25_30MIN_MEAN,TS50_30MIN_MEAN,TS100_30MIN_MEAN,TSS_30MIN_MEAN']\n",
      "Unique stations: 248\n",
      "\n",
      "✓ Saved to: data/imis_all_stations.csv\n",
      "\n",
      "Dataset Overview:\n",
      "Total rows: 84,404,943\n",
      "Total columns: 19\n",
      "\n",
      "Stations included:\n",
      "station_code\n",
      "WFJ2    590676\n",
      "SIM2    507849\n",
      "ROA1    506606\n",
      "VAL2    506510\n",
      "SIM1    506409\n",
      "ROA2    506327\n",
      "KLO2    506275\n",
      "PUZ2    505927\n",
      "NAR2    505874\n",
      "SCH1    505672\n",
      "NAR1    505088\n",
      "KLO1    504576\n",
      "TRU1    504117\n",
      "SCH2    504029\n",
      "PUZ1    503526\n",
      "GAN1    503133\n",
      "TRU2    501712\n",
      "GAN2    501502\n",
      "BED1    501100\n",
      "VAL1    500859\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Create data directory\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "station_url = \"https://measurement-data.slf.ch/imis/data/by_station/\"\n",
    "\n",
    "def fetch_available_stations():\n",
    "    \"\"\"\n",
    "    Fetch list of available station CSV files from SLF\n",
    "    \"\"\"\n",
    "      # <-- Full URL for listing\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(station_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML to find all CSV files\n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all links ending with .csv (but not _pluvio.csv)\n",
    "        csv_files = []\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href', '')\n",
    "            if href.endswith('.csv') and not href.endswith('_pluvio.csv'):\n",
    "                filename = href.split('/')[-1]  # Takes 'ADE2.csv' from '/imis/data/by_station/ADE2.csv'\n",
    "                csv_files.append(filename)  # href is just the filename like 'ADE2.csv'\n",
    "        \n",
    "        print(f\"Found {len(csv_files)} station files\")\n",
    "        return csv_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching station list: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_station_data(station_file):\n",
    "    \"\"\"\n",
    "    Fetch data for a single station CSV file\n",
    "    \"\"\"\n",
    "    # Build full URL: base + path + filename\n",
    "    url = f\"{station_url}{station_file}\"  # <-- FIXED\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Read CSV\n",
    "        df = pd.read_csv(StringIO(response.text), sep=';', low_memory=False)\n",
    "        \n",
    "        # Extract station code from filename (e.g., 'ADE2.csv' -> 'ADE2')\n",
    "        station_code = station_file.replace('.csv', '')\n",
    "        df['station_code'] = station_code\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {station_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_all_imis_data():\n",
    "    \"\"\"\n",
    "    Fetch and combine all IMIS station data\n",
    "    \"\"\"\n",
    "    print(\"Fetching list of available stations...\")\n",
    "    station_files = fetch_available_stations()\n",
    "    \n",
    "    if not station_files:\n",
    "        print(\"No station files found!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nFetching data for {len(station_files)} stations...\")\n",
    "    print(\"This may take several minutes...\\n\")\n",
    "    \n",
    "    all_data = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, station_file in enumerate(station_files, 1):\n",
    "        print(f\"[{i}/{len(station_files)}] Fetching {station_file}...\", end=' ')\n",
    "        \n",
    "        df = fetch_station_data(station_file)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            all_data.append(df)\n",
    "            successful += 1\n",
    "            print(f\"({len(df)} rows)\")\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(\"Failed\")\n",
    "        \n",
    "        # Be respectful to the server\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Data collection complete!\")\n",
    "    print(f\"Successful: {successful}/{len(station_files)}\")\n",
    "    print(f\"Failed: {failed}/{len(station_files)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No data collected!\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    print(\"Combining all station data...\")\n",
    "    df_combined = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"Combined dataset shape: {df_combined.shape}\")\n",
    "    print(f\"Columns: {list(df_combined.columns)}\")\n",
    "    print(f\"Unique stations: {df_combined['station_code'].nunique()}\")\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "# Execute the data collection\n",
    "df_all_stations = fetch_all_imis_data()\n",
    "\n",
    "# Save to CSV\n",
    "if df_all_stations is not None:\n",
    "    output_file = 'data/imis_all_stations.csv'\n",
    "    df_all_stations.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Saved to: {output_file}\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"Total rows: {len(df_all_stations):,}\")\n",
    "    print(f\"Total columns: {len(df_all_stations.columns)}\")\n",
    "    print(f\"\\nStations included:\")\n",
    "    print(df_all_stations['station_code'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7045124",
   "metadata": {},
   "outputs": [],
   "source": [
    "imis_df = pd.read_csv('data/imis_all_stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05b1fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1013, 78)\n"
     ]
    }
   ],
   "source": [
    "df_Engelberg = df[df['station_code'] == 'TIT2']\n",
    "df_Engelberg\n",
    "print(df_Engelberg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "714eaf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_code</th>\n",
       "      <th>measure_date</th>\n",
       "      <th>hyear</th>\n",
       "      <th>TA_30MIN_MEAN</th>\n",
       "      <th>VW_30MIN_MEAN</th>\n",
       "      <th>VW_30MIN_MAX</th>\n",
       "      <th>DW_30MIN_MEAN</th>\n",
       "      <th>RH_30MIN_MEAN</th>\n",
       "      <th>HS</th>\n",
       "      <th>TS0_30MIN_MEAN</th>\n",
       "      <th>TS25_30MIN_MEAN</th>\n",
       "      <th>TS50_30MIN_MEAN</th>\n",
       "      <th>TS100_30MIN_MEAN</th>\n",
       "      <th>RSWR_30MIN_MEAN</th>\n",
       "      <th>TSS_30MIN_MEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>1993-06-01 00:40:00+00:00</td>\n",
       "      <td>1993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>1993-06-01 01:40:00+00:00</td>\n",
       "      <td>1993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>1993-06-01 02:40:00+00:00</td>\n",
       "      <td>1993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>1993-06-01 03:40:00+00:00</td>\n",
       "      <td>1993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>1993-06-01 04:40:00+00:00</td>\n",
       "      <td>1993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400721</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>2025-10-20 01:30:00+00:00</td>\n",
       "      <td>2026</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>101.0</td>\n",
       "      <td>39.1</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400722</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>2025-10-20 02:00:00+00:00</td>\n",
       "      <td>2026</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>111.7</td>\n",
       "      <td>39.4</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400723</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>2025-10-20 02:30:00+00:00</td>\n",
       "      <td>2026</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>104.9</td>\n",
       "      <td>37.1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400724</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>2025-10-20 03:00:00+00:00</td>\n",
       "      <td>2026</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>96.5</td>\n",
       "      <td>39.8</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400725</th>\n",
       "      <td>TIT2</td>\n",
       "      <td>2025-10-20 03:30:00+00:00</td>\n",
       "      <td>2026</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>71.9</td>\n",
       "      <td>40.1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400726 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       station_code               measure_date  hyear  TA_30MIN_MEAN  \\\n",
       "0              TIT2  1993-06-01 00:40:00+00:00   1993            NaN   \n",
       "1              TIT2  1993-06-01 01:40:00+00:00   1993            NaN   \n",
       "2              TIT2  1993-06-01 02:40:00+00:00   1993            NaN   \n",
       "3              TIT2  1993-06-01 03:40:00+00:00   1993            NaN   \n",
       "4              TIT2  1993-06-01 04:40:00+00:00   1993            NaN   \n",
       "...             ...                        ...    ...            ...   \n",
       "400721         TIT2  2025-10-20 01:30:00+00:00   2026            5.9   \n",
       "400722         TIT2  2025-10-20 02:00:00+00:00   2026            5.8   \n",
       "400723         TIT2  2025-10-20 02:30:00+00:00   2026            6.4   \n",
       "400724         TIT2  2025-10-20 03:00:00+00:00   2026            6.1   \n",
       "400725         TIT2  2025-10-20 03:30:00+00:00   2026            6.5   \n",
       "\n",
       "        VW_30MIN_MEAN  VW_30MIN_MAX  DW_30MIN_MEAN  RH_30MIN_MEAN     HS  \\\n",
       "0                 NaN           NaN            NaN            NaN  139.0   \n",
       "1                 NaN           NaN            NaN            NaN  139.0   \n",
       "2                 NaN           NaN            NaN            NaN  139.0   \n",
       "3                 NaN           NaN            NaN            NaN  139.0   \n",
       "4                 NaN           NaN            NaN            NaN  138.0   \n",
       "...               ...           ...            ...            ...    ...   \n",
       "400721            1.8           3.9          101.0           39.1   -1.2   \n",
       "400722            1.9           3.7          111.7           39.4   -1.2   \n",
       "400723            2.5           3.9          104.9           37.1   -0.8   \n",
       "400724            1.6           3.9           96.5           39.8   -1.1   \n",
       "400725            1.6           3.8           71.9           40.1   -0.8   \n",
       "\n",
       "        TS0_30MIN_MEAN  TS25_30MIN_MEAN  TS50_30MIN_MEAN  TS100_30MIN_MEAN  \\\n",
       "0                  NaN              NaN              NaN               NaN   \n",
       "1                  NaN              NaN              NaN               NaN   \n",
       "2                  NaN              NaN              NaN               NaN   \n",
       "3                  NaN              NaN              NaN               NaN   \n",
       "4                  NaN              NaN              NaN               NaN   \n",
       "...                ...              ...              ...               ...   \n",
       "400721             1.9              3.8              4.1               4.6   \n",
       "400722             1.5              3.5              3.9               4.5   \n",
       "400723             2.1              4.9              5.2               5.6   \n",
       "400724             2.2              4.4              4.7               5.0   \n",
       "400725             2.5              5.1              5.3               5.7   \n",
       "\n",
       "        RSWR_30MIN_MEAN  TSS_30MIN_MEAN  \n",
       "0                   NaN             NaN  \n",
       "1                   NaN             NaN  \n",
       "2                   NaN             NaN  \n",
       "3                   NaN             NaN  \n",
       "4                   NaN             NaN  \n",
       "...                 ...             ...  \n",
       "400721              0.0             1.1  \n",
       "400722              0.0             0.6  \n",
       "400723              0.0             2.5  \n",
       "400724              0.0             2.3  \n",
       "400725              0.0             2.7  \n",
       "\n",
       "[400726 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Ebi = pd.read_csv('C:/users/fabio/Downloads/TIT2.csv')\n",
    "df_Ebi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6cec36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: API Approach\n",
      "============================================================\n",
      "\n",
      "Trying station: WFJ\n",
      "Trying: https://measurement-data.slf.ch/api/v1/stations/WFJ/data/2023\n",
      "Trying: https://measurement-data.slf.ch/api/v1/data/station/WFJ\n",
      "Trying: https://measurement-data.slf.ch/imis/options.php?station=WFJ&year=2023\n",
      "No successful endpoints for WFJ\n",
      "No data retrieved for WFJ\n",
      "\n",
      "Trying station: DAV\n",
      "Trying: https://measurement-data.slf.ch/api/v1/stations/DAV/data/2023\n",
      "Trying: https://measurement-data.slf.ch/api/v1/data/station/DAV\n",
      "Trying: https://measurement-data.slf.ch/imis/options.php?station=DAV&year=2023\n",
      "No successful endpoints for DAV\n",
      "No data retrieved for DAV\n",
      "\n",
      "Trying station: ARB\n",
      "Trying: https://measurement-data.slf.ch/api/v1/stations/ARB/data/2023\n",
      "Trying: https://measurement-data.slf.ch/api/v1/data/station/ARB\n",
      "Trying: https://measurement-data.slf.ch/imis/options.php?station=ARB&year=2023\n",
      "No successful endpoints for ARB\n",
      "No data retrieved for ARB\n",
      "\n",
      "Trying station: JFR\n",
      "Trying: https://measurement-data.slf.ch/api/v1/stations/JFR/data/2023\n",
      "Trying: https://measurement-data.slf.ch/api/v1/data/station/JFR\n",
      "Trying: https://measurement-data.slf.ch/imis/options.php?station=JFR&year=2023\n",
      "No successful endpoints for JFR\n",
      "No data retrieved for JFR\n",
      "\n",
      "Trying station: SLF\n",
      "Trying: https://measurement-data.slf.ch/api/v1/stations/SLF/data/2023\n",
      "Trying: https://measurement-data.slf.ch/api/v1/data/station/SLF\n",
      "Trying: https://measurement-data.slf.ch/imis/options.php?station=SLF&year=2023\n",
      "No successful endpoints for SLF\n",
      "No data retrieved for SLF\n",
      "\n",
      "============================================================\n",
      "Method 2: Direct File Download\n",
      "============================================================\n",
      "Trying direct file: https://measurement-data.slf.ch/imis/data/files/AGG.csv\n",
      "Trying direct file: https://measurement-data.slf.ch/imis/data/files/AGG_devicesv\n",
      "Trying direct file: https://measurement-data.slf.ch/imis/data/files/AGG_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "def get_slf_data_aggregated(station_code, year, data_type=\"AGG\"):\n",
    "    \"\"\"\n",
    "    Get SLF data based on the actual structure shown in your image\n",
    "    \"\"\"\n",
    "    # SLF typically uses API endpoints or specific data portals\n",
    "    base_url = \"https://measurement-data.slf.ch/api/v1\"\n",
    "    \n",
    "    # Headers that might be needed for SLF API\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    \n",
    "    # Try different endpoint approaches\n",
    "    endpoints = [\n",
    "        f\"{base_url}/stations/{station_code}/data/{year}\",\n",
    "        f\"{base_url}/data/station/{station_code}\",\n",
    "        f\"https://measurement-data.slf.ch/imis/options.php?station={station_code}&year={year}\"\n",
    "    ]\n",
    "    \n",
    "    for url in endpoints:\n",
    "        print(f\"Trying: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Try to parse as JSON first\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    print(f\"JSON response received for {station_code}\")\n",
    "                    return process_slf_json(data, station_code, year)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Try CSV format\n",
    "                    try:\n",
    "                        df = pd.read_csv(io.StringIO(response.text))\n",
    "                        print(f\"CSV response received for {station_code}\")\n",
    "                        return process_slf_csv(df, station_code, year)\n",
    "                    except:\n",
    "                        print(f\"Could not parse response as JSON or CSV\")\n",
    "                        return None\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"No successful endpoints for {station_code}\")\n",
    "    return None\n",
    "\n",
    "def process_slf_json(data, station_code, year):\n",
    "    \"\"\"Process JSON response from SLF API\"\"\"\n",
    "    # This would need to be adapted based on actual API response\n",
    "    print(f\"Raw JSON keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}\")\n",
    "    return pd.DataFrame()  # Placeholder\n",
    "\n",
    "def process_slf_csv(df, station_code, year):\n",
    "    \"\"\"Process CSV response from SLF\"\"\"\n",
    "    print(f\"CSV columns: {list(df.columns)}\")\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    \n",
    "    # Handle European number format (comma as decimal separator)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Try to convert comma decimals to dot decimals\n",
    "            try:\n",
    "                df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_slf_data_direct_files():\n",
    "    \"\"\"\n",
    "    Alternative approach: Direct file download based on your image structure\n",
    "    \"\"\"\n",
    "    # From your image, it seems files are named AGG.csv\n",
    "    base_files_url = \"https://measurement-data.slf.ch/imis/data/files/\"\n",
    "    \n",
    "    files_to_try = [\n",
    "        \"AGG.csv\",\n",
    "        \"AGG_devicesv\", \n",
    "        f\"AGG_{datetime.now().year}.csv\"\n",
    "    ]\n",
    "    \n",
    "    for filename in files_to_try:\n",
    "        url = f\"{base_files_url}{filename}\"\n",
    "        print(f\"Trying direct file: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                df = pd.read_csv(io.StringIO(response.text))\n",
    "                print(f\"Successfully downloaded {filename}\")\n",
    "                return df\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def test_slf_download():\n",
    "    \"\"\"Test SLF data download with realistic approaches\"\"\"\n",
    "    \n",
    "    # Stations that likely exist in SLF system\n",
    "    test_stations = [\n",
    "        \"WFJ\",  # Weissfluhjoch\n",
    "        \"DAV\",  # Davos\n",
    "        \"ARB\",  # Arosa\n",
    "        \"JFR\",  # Jungfraujoch\n",
    "        \"SLF\",  # SLF headquarters\n",
    "    ]\n",
    "    \n",
    "    test_year = 2023\n",
    "    \n",
    "    print(\"Method 1: API Approach\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for station in test_stations:\n",
    "        print(f\"\\nTrying station: {station}\")\n",
    "        data = get_slf_data_aggregated(station, test_year)\n",
    "        \n",
    "        if data is not None and not data.empty:\n",
    "            print(f\"Success! Data shape: {data.shape}\")\n",
    "            print(f\"Columns: {list(data.columns)}\")\n",
    "            print(data.head())\n",
    "        else:\n",
    "            print(f\"No data retrieved for {station}\")\n",
    "        \n",
    "        time.sleep(2)  # Be respectful to the server\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Method 2: Direct File Download\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    direct_data = get_slf_data_direct_files()\n",
    "    if direct_data is not None:\n",
    "        print(f\"Direct file data shape: {direct_data.shape}\")\n",
    "        print(direct_data.head())\n",
    "\n",
    "# Additional function to handle the specific format from your image\n",
    "def parse_slf_gainers_data(csv_text):\n",
    "    \"\"\"\n",
    "    Parse data in the specific 'Gainers | Cells' format from your image\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(io.StringIO(csv_text), sep='|')\n",
    "    \n",
    "    # Clean column names and data\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    \n",
    "    # Handle the specific format you showed\n",
    "    if 'Gainers' in df.columns and 'Cells' in df.columns:\n",
    "        # Convert European number format\n",
    "        df['Cells'] = df['Cells'].str.replace(',', '.').astype(float)\n",
    "        \n",
    "        # Parse the Gainers column (appears to be dates/identifiers)\n",
    "        df['Gainers_parsed'] = df['Gainers'].str.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    test_slf_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b047d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching bulletins from 2014 to 2024 for Davos region...\n",
      "Date range: 2014-11-01 to 2024-05-31\n",
      "\n",
      "Progress: 2023-11-14 - Records collected: 14\n",
      "Progress: 2023-11-14 - Records collected: 14\n",
      "Progress: 2024-02-22 - Records collected: 114\n",
      "Progress: 2024-02-22 - Records collected: 114\n",
      "\n",
      "============================================================\n",
      "Data collection complete!\n",
      "Total records: 209\n",
      "Failed dates: 3294\n",
      "DataFrame shape: (209, 7)\n",
      "Date range in data: 2023-11-01 to 2024-05-31\n",
      "Unique region names found: 142\n",
      "First few region names: ['Génépi', 'nördliches Tujetsch', 'Jungfrau - Schilthorn', 'Bernina', 'Stoos', 'Toggenburg', 'Blüemlisalp', 'Saint-Cergue', 'Maderanertal', 'Val dal Spöl']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Data collection complete!\n",
      "Total records: 209\n",
      "Failed dates: 3294\n",
      "DataFrame shape: (209, 7)\n",
      "Date range in data: 2023-11-01 to 2024-05-31\n",
      "Unique region names found: 142\n",
      "First few region names: ['Génépi', 'nördliches Tujetsch', 'Jungfrau - Schilthorn', 'Bernina', 'Stoos', 'Toggenburg', 'Blüemlisalp', 'Saint-Cergue', 'Maderanertal', 'Val dal Spöl']\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>regions</th>\n",
       "      <th>region_ids</th>\n",
       "      <th>danger_level</th>\n",
       "      <th>elevation_lower</th>\n",
       "      <th>elevation_upper</th>\n",
       "      <th>aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>Bex-Villars, Wildhorn, Iffigen, Engstligen, Bl...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>Gadmertal, Engelberg, Schächental, Uri Rot Sto...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , ,</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-03</td>\n",
       "      <td>Gadmertal, Engelberg, Schächental, Uri Rot Sto...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>Engstligen, Blüemlisalp, Jungfrau - Schilthorn...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>Waadtländer Voralpen, Jaun, Hohgant, Niedersim...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-11-06</td>\n",
       "      <td>Waadtländer Voralpen, Jaun, Hohgant, Niedersim...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>Pays d'Enhaut, Aigle-Leysin, Gstaad, Lenk, Ade...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-11-08</td>\n",
       "      <td>Pays d'Enhaut, Aigle-Leysin, Gstaad, Lenk, Ade...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-11-09</td>\n",
       "      <td>Pays d'Enhaut, Aigle-Leysin, Gstaad, Lenk, Eng...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-11-10</td>\n",
       "      <td>Ybrig, Stoos, Bisistal, Glarus Nord, Glarus Sü...</td>\n",
       "      <td>, , , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "      <td>moderate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                            regions  \\\n",
       "0  2023-11-01  Bex-Villars, Wildhorn, Iffigen, Engstligen, Bl...   \n",
       "1  2023-11-02  Gadmertal, Engelberg, Schächental, Uri Rot Sto...   \n",
       "2  2023-11-03  Gadmertal, Engelberg, Schächental, Uri Rot Sto...   \n",
       "3  2023-11-04  Engstligen, Blüemlisalp, Jungfrau - Schilthorn...   \n",
       "4  2023-11-05  Waadtländer Voralpen, Jaun, Hohgant, Niedersim...   \n",
       "5  2023-11-06  Waadtländer Voralpen, Jaun, Hohgant, Niedersim...   \n",
       "6  2023-11-07  Pays d'Enhaut, Aigle-Leysin, Gstaad, Lenk, Ade...   \n",
       "7  2023-11-08  Pays d'Enhaut, Aigle-Leysin, Gstaad, Lenk, Ade...   \n",
       "8  2023-11-09  Pays d'Enhaut, Aigle-Leysin, Gstaad, Lenk, Eng...   \n",
       "9  2023-11-10  Ybrig, Stoos, Bisistal, Glarus Nord, Glarus Sü...   \n",
       "\n",
       "                                          region_ids danger_level  \\\n",
       "0  , , , , , , , , , , , , , , , , , , , , , , , ...     moderate   \n",
       "1       , , , , , , , , , , , , , , , , , , , , , ,      moderate   \n",
       "2  , , , , , , , , , , , , , , , , , , , , , , , ...     moderate   \n",
       "3  , , , , , , , , , , , , , , , , , , , , , , , ...     moderate   \n",
       "4  , , , , , , , , , , , , , , , , , , , , , , , ...     moderate   \n",
       "5  , , , , , , , , , , , , , , , , , , , , , , , ...     moderate   \n",
       "6  , , , , , , , , , , , , , , , , , , , , , , , ...     moderate   \n",
       "7  , , , , , , , , , , , , , , , , , , , , , , , ...     moderate   \n",
       "8  , , , , , , , , , , , , , , , , , , , , , , , ...     moderate   \n",
       "9  , , , , , , , , , , , , , , , , , , , , , , , ...     moderate   \n",
       "\n",
       "  elevation_lower elevation_upper aspects  \n",
       "0            None            None          \n",
       "1            None            None          \n",
       "2            None            None          \n",
       "3            None            None          \n",
       "4            None            None          \n",
       "5            None            None          \n",
       "6            None            None          \n",
       "7            None            None          \n",
       "8            None            None          \n",
       "9            None            None          "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch all bulletins from 2014-2024 for Davos region\n",
    "start_date = datetime(2014, 11, 1)  # Winter season starts in November\n",
    "end_date = datetime(2024, 5, 31)    # Winter season ends in May\n",
    "current_date = start_date\n",
    "\n",
    "# Extract data into list of records\n",
    "records = []\n",
    "failed_dates = []\n",
    "all_region_names = set()  # To track unique region names\n",
    "\n",
    "print(\"Fetching bulletins from 2014 to 2024 for Davos region...\")\n",
    "print(f\"Date range: {start_date.date()} to {end_date.date()}\\n\")\n",
    "\n",
    "# Iterate through all dates\n",
    "while current_date <= end_date:\n",
    "    try:\n",
    "        bulletin_json = fetch_bulletin_json(current_date)\n",
    "        \n",
    "        # Check if bulletins exist\n",
    "        if not bulletin_json.get('bulletins'):\n",
    "            failed_dates.append(current_date.date())\n",
    "            current_date += timedelta(days=1)\n",
    "            continue\n",
    "        \n",
    "        for bulletin in bulletin_json.get('bulletins', []):\n",
    "            regions = bulletin.get('regions', [])\n",
    "            \n",
    "            # Collect region names and IDs\n",
    "            region_names = []\n",
    "            region_ids = []\n",
    "            for r in regions:\n",
    "                name = r.get('name', '')\n",
    "                region_id = r.get('regionId', '')\n",
    "                region_names.append(name)\n",
    "                region_ids.append(region_id)\n",
    "                all_region_names.add(name)\n",
    "            \n",
    "            # Filter for Davos region - check both name and ID\n",
    "            # Davos region ID is typically \"CH-7114\" or similar\n",
    "            is_davos = any(\n",
    "                'davos' in name.lower() or \n",
    "                'davos' in rid.lower() or\n",
    "                'CH-7114' in rid or\n",
    "                'CH-7115' in rid\n",
    "                for name, rid in zip(region_names, region_ids)\n",
    "            )\n",
    "            \n",
    "            if is_davos:\n",
    "                danger_ratings = bulletin.get('dangerRatings', [])\n",
    "                for rating in danger_ratings:\n",
    "                    records.append({\n",
    "                        'date': current_date.date(),\n",
    "                        'regions': ', '.join(region_names),\n",
    "                        'region_ids': ', '.join(region_ids),\n",
    "                        'danger_level': rating.get('mainValue', 'N/A'),\n",
    "                        'elevation_lower': rating.get('validElevation', {}).get('lowerBound'),\n",
    "                        'elevation_upper': rating.get('validElevation', {}).get('upperBound'),\n",
    "                        'aspects': ', '.join(rating.get('aspects', []))\n",
    "                    })\n",
    "        \n",
    "        # Print progress every 100 days\n",
    "        if (current_date - start_date).days % 100 == 0:\n",
    "            print(f\"Progress: {current_date.date()} - Records collected: {len(records)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_dates.append(current_date.date())\n",
    "    \n",
    "    # Move to next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_bulletins = pd.DataFrame(records)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Data collection complete!\")\n",
    "print(f\"Total records: {len(records)}\")\n",
    "print(f\"Failed dates: {len(failed_dates)}\")\n",
    "print(f\"DataFrame shape: {df_bulletins.shape}\")\n",
    "if len(df_bulletins) > 0:\n",
    "    print(f\"Date range in data: {df_bulletins['date'].min()} to {df_bulletins['date'].max()}\")\n",
    "print(f\"Unique region names found: {len(all_region_names)}\")\n",
    "print(f\"First few region names: {list(all_region_names)[:10]}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "df_bulletins.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avalanche_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
