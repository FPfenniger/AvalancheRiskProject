{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bada8c77",
   "metadata": {},
   "source": [
    "# **Avalanche Risk Project**\n",
    "\n",
    "Advanced Data Analytics, Fall 2025\n",
    "\n",
    "The following project examines the feasibility of machine learning models to predict avalanche danger from spatial and meteorological features for the Davos Valley. In a second step, the model will be trained on the whole of Switzerland and tested as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bbc72",
   "metadata": {},
   "source": [
    "### **1. Data Import**\n",
    "\n",
    "I start by importing all necessary data from the different APIs including:\n",
    "\n",
    "- SLF Bulletin Archive\n",
    "- Meteo Swiss IMIS Data Archive\n",
    "- SwissTopo Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "493da979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Libraries \n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3082246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bulletins: 9\n",
      "\n",
      "==================================================\n",
      "First bulletin keys:\n",
      "{\n",
      "  \"bulletinID\": \"a7480e81-950a-4036-856c-a5497803260d\",\n",
      "  \"validTime\": {\n",
      "    \"startTime\": \"2024-01-15T07:00:00Z\",\n",
      "    \"endTime\": \"2024-01-15T16:00:00Z\"\n",
      "  },\n",
      "  \"nextUpdate\": \"2024-01-15T16:00:00Z\",\n",
      "  \"publicationTime\": \"2024-01-15T06:50:08.489548751Z\",\n",
      "  \"lang\": \"en\",\n",
      "  \"regions\": [\n",
      "    {\n",
      "      \"regionID\": \"CH-4244\",\n",
      "      \"name\": \"s\\u00fcdliches Obergoms\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-2223\",\n",
      "      \"name\": \"n\\u00f6rdliches Urseren\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-1312\",\n",
      "      \"name\": \"Monthey-Val d'Illiez\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4242\",\n",
      "      \"name\": \"Binntal\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4231\",\n",
      "      \"name\": \"n\\u00f6rdliches Simplon Gebiet\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4114\",\n",
      "      \"name\": \"Conthey-Fully\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-1245\",\n",
      "      \"name\": \"Guttannen\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4241\",\n",
      "      \"name\": \"Reckingen\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-1114\",\n",
      "      \"name\": \"Bex-Villars\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4213\",\n",
      "      \"name\": \"Konkordia Gebiet\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4214\",\n",
      "      \"name\": \"Riederalp\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-5124\",\n",
      "      \"name\": \"Flims\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-1247\",\n",
      "      \"name\": \"Grimselpass\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-1243\",\n",
      "      \"name\": \"Schreckhorn\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4121\",\n",
      "      \"name\": \"Montana\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4215\",\n",
      "      \"name\": \"Leuk\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-5211\",\n",
      "      \"name\": \"n\\u00f6rdliches Tujetsch\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4243\",\n",
      "      \"name\": \"n\\u00f6rdliches Obergoms\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-4211\",\n",
      "      \"name\": \"Leukerbad - L\\u00f6tschental\"\n",
      "    },\n",
      "    {\n",
      "      \"regionID\": \"CH-2224\",\n",
      "      \"name\": \"s\\u00fcdliches Urseren\"\n",
      "    }\n",
      "  ],\n",
      "  \"dangerRatings\": [\n",
      "    {\n",
      "      \"mainValue\": \"considerable\",\n",
      "      \"validTimePeriod\": \"all_day\",\n",
      "      \"customData\": {\n",
      "        \"CH\": {\n",
      "          \"subdivision\": \"minus\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"avalancheProblems\": [\n",
      "  \n",
      "\n",
      "==================================================\n",
      "Extracting danger ratings...\n",
      "\n",
      "\n",
      "Bulletin 1:\n",
      "  Danger Level: considerable\n",
      "  Elevation: {}\n",
      "  Regions: südliches Obergoms, nördliches Urseren, Monthey-Val d'Illiez...\n",
      "\n",
      "Bulletin 2:\n",
      "  Danger Level: considerable\n",
      "  Elevation: {}\n",
      "  Regions: Zermatt, Saas Fee, Val d'Hérens...\n",
      "\n",
      "Bulletin 3:\n",
      "  Danger Level: considerable\n",
      "  Elevation: {}\n",
      "  Regions: Gadmertal, Iffigen, Gstaad...\n",
      "\n",
      "Bulletin 4:\n",
      "  Danger Level: moderate\n",
      "  Elevation: {}\n",
      "  Regions: östliche Berner Voralpen, Toggenburg, Appenzeller Alpen...\n",
      "\n",
      "Bulletin 5:\n",
      "  Danger Level: moderate\n",
      "  Elevation: {}\n",
      "  Regions: Münstertal, unteres Puschlav, Corvatsch...\n",
      "\n",
      "Bulletin 6:\n",
      "  Danger Level: moderate\n",
      "  Elevation: {}\n",
      "  Regions: Vouvry, Hohgant, Glarus Nord...\n",
      "\n",
      "Bulletin 7:\n",
      "  Danger Level: moderate\n",
      "  Elevation: {}\n",
      "  Regions: basso Moesano, obere Leventina, untere Leventina...\n",
      "\n",
      "Bulletin 8:\n",
      "  Danger Level: low\n",
      "  Elevation: {}\n",
      "  Regions: Yverdon - Bevaix, Val de Travers, Moutier - Tavannes...\n",
      "\n",
      "Bulletin 9:\n",
      "  Danger Level: low\n",
      "  Elevation: {}\n",
      "  Regions: Luganese, Riviera, Mendrisiotto...\n"
     ]
    }
   ],
   "source": [
    "# Get the Bulletin Data from SLF API\n",
    "def fetch_bulletin_json(date, lang='en'):\n",
    "    url = f\"https://aws.slf.ch/api/bulletin/caaml/{lang}/json\"\n",
    "    params = {'activeAt': date.strftime('%Y-%m-%dT08:00:00+01:00')}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# Fetch a winter day with actual danger\n",
    "historical = fetch_bulletin_json(datetime(2024, 1, 15), lang='en')\n",
    "\n",
    "print(f\"Number of bulletins: {len(historical['bulletins'])}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Look at first bulletin structure\n",
    "first_bulletin = historical['bulletins'][0]\n",
    "print(\"First bulletin keys:\")\n",
    "print(json.dumps(first_bulletin, indent=2)[:2000])\n",
    "\n",
    "# Try to find danger ratings\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Extracting danger ratings...\\n\")\n",
    "\n",
    "for i, bulletin in enumerate(historical['bulletins']):\n",
    "    print(f\"\\nBulletin {i+1}:\")\n",
    "    \n",
    "    # Look for danger ratings\n",
    "    if 'dangerRatings' in bulletin:\n",
    "        for rating in bulletin['dangerRatings']:\n",
    "            danger_level = rating.get('mainValue', 'N/A')\n",
    "            regions = rating.get('validElevation', {})\n",
    "            \n",
    "            print(f\"  Danger Level: {danger_level}\")\n",
    "            print(f\"  Elevation: {regions}\")\n",
    "    \n",
    "    # Get regions\n",
    "    if 'regions' in bulletin:\n",
    "        region_names = [r.get('name', 'Unknown') for r in bulletin['regions'][:3]]\n",
    "        print(f\"  Regions: {', '.join(region_names)}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3675d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "ExpatError",
     "evalue": "mismatched tag: line 391, column 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mExpatError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxmltodict\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Umwandeln von XML in Dict\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m caaml_dict = \u001b[43mxmltodict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Schauen, wie die Daten strukturiert sind\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(caaml_dict.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fabio\\miniconda3\\envs\\avalanche_project\\Lib\\site-packages\\xmltodict.py:368\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(xml_input, encoding, expat, process_namespaces, namespace_separator, disable_entities, process_comments, **kwargs)\u001b[39m\n\u001b[32m    366\u001b[39m     parser.Parse(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m handler.item\n",
      "\u001b[31mExpatError\u001b[39m: mismatched tag: line 391, column 2"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "\n",
    "# Umwandeln von XML in Dict\n",
    "caaml_dict = xmltodict.parse(response.content)\n",
    "\n",
    "# Schauen, wie die Daten strukturiert sind\n",
    "print(caaml_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b279bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CONFIGURATION ---------------------------------------------------\n",
    "REGION_NAME = \"Davos\"                   # region you work on\n",
    "START_DATE = \"2022-11-01\"               # example start\n",
    "END_DATE   = \"2023-04-30\"               # example end\n",
    "STATION_IDS = [\"WFJ2\", \"DAV2\"]          # example station codes for Davos region\n",
    "\n",
    "# STAC base URL for MeteoSwiss via FSDI\n",
    "STAC_BASE = \"https://data.geo.admin.ch/api/stac/v1\"\n",
    "\n",
    "# Collection names for station data (example)\n",
    "COL_AUTO_WS  = \"ch.meteoschweiz.ogd-smn\"      # automatic weather stations (temp, wind, etc) :contentReference[oaicite:3]{index=3}\n",
    "COL_PRECIP   = \"ch.meteoschweiz.ogd-smn-precip\"  # automatic precipitation stations :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "# Bulletin endpoint placeholder (you’ll need to confirm actual URL)\n",
    "BULLETIN_URL = \"https://www.slf.ch/fileadmin/content/lawinenbulletin/daten/json/bulletin.json\"\n",
    "\n",
    "# --- FUNCTIONS ------------------------------------------------------\n",
    "\n",
    "def fetch_station_data(collection, station_id, start_date, end_date):\n",
    "    \"\"\"Fetch station data for a specific station from STAC API.\"\"\"\n",
    "    params = {\n",
    "        \"time\": f\"{start_date}T00:00:00Z/{end_date}T23:59:59Z\",\n",
    "        \"properties\": f\"station:{station_id}\"\n",
    "    }\n",
    "    url = f\"{STAC_BASE}/collections/{collection}/items\"\n",
    "    resp = requests.get(url, params=params)\n",
    "    resp.raise_for_status()\n",
    "    features = resp.json().get(\"features\", [])\n",
    "    # convert to DataFrame: flatten each feature’s assets etc\n",
    "    records = []\n",
    "    for feat in features:\n",
    "        rec = {\n",
    "            \"time\": feat[\"properties\"][\"datetime\"],\n",
    "            \"station_id\": station_id\n",
    "        }\n",
    "        # You may want to parse assets or properties depending on dataset\n",
    "        # rec[\"temp\"] = feat[\"properties\"].get(\"t2m\")\n",
    "        records.append(rec)\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    return df\n",
    "\n",
    "def fetch_bulletin_data():\n",
    "    \"\"\"Fetch the bulletin JSON and extract region danger levels.\"\"\"\n",
    "    resp = requests.get(BULLETIN_URL)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    # You’ll need to inspect structure → Example:\n",
    "    # regions_data = data[\"regions\"]\n",
    "    # Filter for REGION_NAME\n",
    "    rows = []\n",
    "    for r in data.get(\"regions\", []):\n",
    "        if r.get(\"region_name\") == REGION_NAME:\n",
    "            rows.append({\n",
    "                \"date\": pd.to_datetime(r[\"date\"]),\n",
    "                \"danger_level\": r[\"danger_level\"]\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- MAIN SCRIPT ----------------------------------------------------\n",
    "\n",
    "# 1. Fetch danger level (target)\n",
    "df_danger = fetch_bulletin_data()\n",
    "print(\"Danger levels:\", df_danger.head())\n",
    "\n",
    "# 2. Fetch station feature data (loop over collections & stations)\n",
    "df_list = []\n",
    "for station in STATION_IDS:\n",
    "    df_temp = fetch_station_data(COL_AUTO_WS, station, START_DATE, END_DATE)\n",
    "    df_prec = fetch_station_data(COL_PRECIP, station, START_DATE, END_DATE)\n",
    "    # merge or pivot as needed\n",
    "    df_station = df_temp.merge(df_prec, on=[\"time\",\"station_id\"], how=\"outer\")\n",
    "    df_list.append(df_station)\n",
    "\n",
    "df_features = pd.concat(df_list, axis=0).reset_index(drop=True)\n",
    "print(\"Features data:\", df_features.head())\n",
    "\n",
    "# 3. Merge features + danger by date\n",
    "df_features[\"date\"] = df_features[\"time\"].dt.date\n",
    "df_danger[\"date\"] = df_danger[\"date\"].dt.date\n",
    "df_merged = pd.merge(df_features, df_danger, on=\"date\", how=\"left\")\n",
    "\n",
    "print(\"Merged dataset sample:\\n\", df_merged.head())\n",
    "\n",
    "# 4. Save to CSV\n",
    "df_merged.to_csv(\"data/davos_features_danger.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avalanche_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
