{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc79b42e",
   "metadata": {},
   "source": [
    "# **Avalanche Risk Project**\n",
    "\n",
    "Advanced Data Analytics, Fall 2025\n",
    "\n",
    "The following project examines the feasibility of deep learning models (3D-CNN, convLSTM) compared to traditional, state-of-the-art methods for predicting avalanche danger levels. Using a comprehensive dataset from the Swiss Federal Institute for Snow and Avalanche Research (SLF) spanning from 1997-2020, the project heavily relies on the work of Pérez-Guillén et al. (2022) and Maissen et al. (2024) and adopts many of their preprocessing and modeling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09474741",
   "metadata": {},
   "source": [
    "**1. Training Phase (The CNN learns \"Pixels\")**\n",
    "Input: The 7-day Data Cube (Weather + Terrain).\n",
    "\n",
    "Model: The 3D CNN processes this.\n",
    "\n",
    "Output: It predicts a Danger Map (e.g., a 180x370 grid where every pixel has a value like 2.3 or 3.1).\n",
    "\n",
    "Loss Function: The model compares its pixel predictions against your Target Raster (from Script 3) to learn.\n",
    "\n",
    "Note: You do NOT need aggregation here. The model learns best by trying to get every single pixel right.\n",
    "\n",
    "**2. Inference Phase (The \"Aggregation\" Step)**\n",
    "When: This happens when you want to evaluate your model on the Test Set or generate a final forecast.\n",
    "\n",
    "Input: The Danger Map produced by the CNN.\n",
    "\n",
    "Action: You run the aggregate_predictions script.\n",
    "\n",
    "It overlays the SLF Region Mask.\n",
    "\n",
    "It calculates the 90th percentile for \"Region 15\".\n",
    "\n",
    "Output: A final dictionary: {'Region_15': 3, 'Region_16': 2, ...}.\n",
    "\n",
    "**3. Comparison Phase (The Final Grade)**\n",
    "You take that final dictionary (Regional Forecast).\n",
    "\n",
    "You compare it against the Original CSV Ground Truth (not the raster).\n",
    "\n",
    "This gives you your final Macro-F1 Score to compare against the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1f5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b76784",
   "metadata": {},
   "source": [
    "## **5. CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10575a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DYNAMIC_FEATURES = [\n",
    "    # --- 1. The \"Super Predictors\" (Top Correlations) ---\n",
    "    'delta_elevation',    # Rank #2 (r=-0.601). Critical context.\n",
    "    'Pen_depth',          # Rank #3 (r=+0.581). Best proxy for \"unstable new snow\".\n",
    "    'HN72_24',            # Rank #4 (r=+0.543). Best measure of cumulative loading.\n",
    "\n",
    "    # --- 2. The \"Driver\" Group (Weather) ---\n",
    "    'TA',                 # Rank #7 (r=-0.420). Temperature is the main \"state\" variable.\n",
    "    'wind_trans24',       # Rank #8-ish. Captures the \"loading\" effect better than raw wind.\n",
    "    'RH',                 # Rank #13 (r=+0.360). Proxy for storm presence.\n",
    "\n",
    "    # --- 3. The \"Stability\" Group (Snowpack Structure) ---\n",
    "    # We chose min_ccl_pen over Sn because your EDA showed it has higher correlation\n",
    "    # (r=-0.374 vs r=-0.292).\n",
    "    'min_ccl_pen',        \n",
    "\n",
    "    # --- 4. The \"Engineered\" Context ---\n",
    "    'relative_load_3d',   # You proved this had strong correlation (+0.410) in feature engineering.\n",
    "    \n",
    "    # --- 5. Raw Wind (Required for CNN Spatial Learning) ---\n",
    "    # Even though raw 'VW' had lower correlation, the CNN NEEDS vectors to learn \n",
    "    # \"Lee Slope Loading\" patterns spatially.\n",
    "    # Note: The script adds 'wind_u' and 'wind_v' automatically from VW/DW.\n",
    "    'VW', 'DW' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "GRID_DIR = '../data/grids'\n",
    "DYNAMIC_DIR = os.path.join(GRID_DIR, 'dynamic')\n",
    "TARGET_DIR = os.path.join(GRID_DIR, 'targets')\n",
    "STATIC_FILE = os.path.join(GRID_DIR, 'static_terrain.npy')\n",
    "\n",
    "# Must match your Script 2 generation order!\n",
    "FEATURE_NAMES = [\n",
    "    'delta_elevation', 'Pen_depth', 'HN72_24',  # Top Predictors\n",
    "    'TA', 'wind_trans24', 'RH',                 # Weather Drivers\n",
    "    'min_ccl_pen',                              # Stability\n",
    "    'relative_load_3d',                         # Engineered\n",
    "    'wind_u', 'wind_v'                          # Vectors\n",
    "]\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 4        # Small batch size because 3D tensors are huge in RAM\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 10\n",
    "LOOKBACK = 7          # T-6 to T\n",
    "NUM_CLASSES = 5       # 0=NoData (Ignore), 1, 2, 3, 4 (High)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Running on device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvalancheDataset(Dataset):\n",
    "    def __init__(self, date_list, feature_dir, target_dir, static_file, stats, features, lookback=7):\n",
    "        self.dates = date_list\n",
    "        self.feature_dir = feature_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.stats = stats\n",
    "        self.features = features\n",
    "        self.lookback = lookback\n",
    "        \n",
    "        # Load static data once (H, W, 4) -> (Elevation, Slope, AspectSin, AspectCos)\n",
    "        self.static_data = np.load(static_file).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_date = pd.to_datetime(self.dates[idx])\n",
    "        \n",
    "        # 1. Build Dynamic Sequence (T-6 to T) \n",
    "        frames = []\n",
    "        start_date = target_date - pd.Timedelta(days=self.lookback - 1)\n",
    "        \n",
    "        for i in range(self.lookback):\n",
    "            d_str = (start_date + pd.Timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "            f_path = os.path.join(self.feature_dir, f\"{d_str}.npz\")\n",
    "            \n",
    "            if os.path.exists(f_path):\n",
    "                # Load raw data (H, W, C)\n",
    "                raw = np.load(f_path)['data']\n",
    "                norm = np.zeros_like(raw)\n",
    "                # Normalize channel-wise\n",
    "                for c, name in enumerate(self.features):\n",
    "                    mu, std = self.stats[name]['mean'], self.stats[name]['std']\n",
    "                    norm[..., c] = (raw[..., c] - mu) / (std + 1e-6)\n",
    "                frames.append(norm)\n",
    "            else:\n",
    "                # Missing day? Fill with zeros\n",
    "                frames.append(np.zeros_like(self.static_data[..., :len(self.features)]))\n",
    "        \n",
    "        # Stack -> (Time, H, W, C_dyn)\n",
    "        dynamic_tensor = np.stack(frames, axis=0)\n",
    "\n",
    "        # --- 2. Add Static Data ---\n",
    "        # Repeat static data for every timestep\n",
    "        static_expanded = np.tile(self.static_data[np.newaxis, ...], (self.lookback, 1, 1, 1))\n",
    "        \n",
    "        # Combine -> (Time, H, W, Total_Channels)\n",
    "        full_cube = np.concatenate([dynamic_tensor, static_expanded], axis=-1)\n",
    "\n",
    "        # --- 3. Load Target ---\n",
    "        t_path = os.path.join(self.target_dir, f\"{self.dates[idx]}.npy\")\n",
    "        if os.path.exists(t_path):\n",
    "            # Background is -1, Classes are 1,2,3,4.\n",
    "            # We shift classes to 0,1,2,3,4 for PyTorch? \n",
    "            # Actually, simpler: Let's map -1 -> 0 (Ignore), 1->1, etc.\n",
    "            label = np.load(t_path).astype(np.int64)\n",
    "            label[label == -1] = 0 # Move \"No Data\" to class 0\n",
    "        else:\n",
    "            label = np.zeros((self.static_data.shape[0], self.static_data.shape[1]), dtype=np.int64)\n",
    "\n",
    "        # --- 4. Permute for PyTorch ---\n",
    "        # Input: (Channels, Time, Height, Width)\n",
    "        X = torch.from_numpy(full_cube).permute(3, 0, 1, 2)\n",
    "        Y = torch.from_numpy(label)\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvalancheNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AvalancheNet, self).__init__()\n",
    "        \n",
    "        # 3D Convolutions: Process (Time, Height, Width)\n",
    "        # padding='same' keeps the output map size equal to input size\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        # Collapse Time Dimension: Average over the 7 days\n",
    "        # Output becomes (Batch, 64, H, W)\n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        \n",
    "        # Final 2D Convolutions to map to classes\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, C, T, H, W)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Aggregate Time: (Batch, 64, 1, H, W) -> Squeeze -> (Batch, 64, H, W)\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        \n",
    "        # Predict Classes\n",
    "        x = self.final_conv(x) # (Batch, Num_Classes, H, W)\n",
    "        return x"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAB1CAYAAAC2yK9pAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAABV8SURBVHhe7d1fSCPX2wfw7+99cQtJusoaV7smbAKtLSTKXFTqXZGwEBjsTW4CKg1eeVUEdyF0Ed7FbhHWBd0roSCCuxCkoRciSJcghRaE3ExXhbJ7YUrc1jRxsSW6RQvve/HmHGZOMnNmkvhnt88HvDAnZ3LmZPLknMmZZ/5z8+bN/wUhhFj4L/EBQggRUaAghEhRoCCESFGgIIRIOQoUiqJgeXkZiURCLLqULrq9qqoilUohnU4jnU5fWDsIaZSjQEGcWVtbQzwex/T0NI6Pj8ViQt4YFCgIIVL/ka2jSCQSGBoaMjy2urqKpaWlmuW7u7u4ffs2ACCZTMLr9aJUKqG/v7+qnD2HlTHHx8d4+PAhNE2r2r7+tVVVRSwWQzqdRjweh8vlMtRVFAWTk5PIZDK8jshs+6zuzs4OZmZmeHkymUQoFLLVPsZOOwi5zCxHFIlEAtFoFIuLi4jFYlVDaEVRcPPmTcRiMcRiMSwuLsLn8xnm4sFgEF6vt2Z5IpGAoih8+6urq1VBQv/6i4uLiEajhu27XC6MjY0hk8kgFouhUChgZGSEl1ux2r6madjZ2UEgEIDf7wcA+P1+BAIB7Ozs2G4fIW8Dy0ARDoehaRrW1tbEIgCApmm4d+8e///Zs2c4PDxEV1cXf6xYLGJ+fr5meVdXF/b29vj2NU3D6ekpuru7gRqvv7a2hr29PYTDYb59CN/ipVIJHo+Hf7ityLa/tbUFt9uNvr4+AEBfXx+uXLmC9fV1W/UJeVuYBgq/3w+PxyM+XGV2dpaf1Z+bm0NHR4f4FFP7+/vw+XxQVRUAEI1G0dLSgpcvX/LX7+/v59tPp9MIBoOGbZyenqJYLPL/Z2ZmMD4+jnw+b3ieyM7219bWUCgU0NvbCwDo7e3Fq1evoGmarfqEvC1MA4UdyWQSnZ2dmJ6eRiwWw8TEhOFDa0dLSwvGxsaQTqehKApSqRQ0TUM+n0e5XEY2m+VTG/anP8dRL7vb397eRiAQgKIouHHjBjY2NhzVJ+RtYBoo8vk8crkcn6OzE3Iul4s/x+v14ujoCAcHBwCA4eFhRyOKcDhs+KDF43HDNGd7exuKovARR7PZ2X4mk8HJyQkGBweByvSJsVOfkLeB5a8efr8fd+/e5R/+p0+fQlEUbG5uGn4ZYMFjd3cXqJwnmJmZQTKZRCAQwP3795HP5/n2crkcZmZmoKoqRkdH0dLSYnhdq19V9OWqqiIejyOVStU8jyK2j8lms/yXDKvtM+yXGfFxSOrX+kUHwusT8iawDBRnSQwajPjzIyHk4plOPc5ae3s73G63+HDVdIYQcvEubESByoIpcepRLBb5VIUQcjlcaKAghLwZLmzqQQh5c1CgIIRIUaAghEhRoCCESFGgIIRIUaAghEhJfx6dnZ3lV0Tqc0XYJS5xrrUM+iI1q31suXat5dlmZeI6ktPTUywvL1ctR6+3fq0l7PXuH/l3sxxRsAQs7KKtQqGA8fFxW7keUDmQI5GIITFNNBq9NBdRNat9qqoiFApV5cX0+/1YWFjA1atXq8qgy6nJ+lfTNAwNDRkS5TRSX9M0jI6O8vLV1VVEIhEoiiJuihBLloFiaWnJcMn0xsaGIZGLzODgIAqFAv+Gy2QyODw85FdiXrRmtW9wcBA7OzsoFAqGx4eHh7G5uYmVlRXD42b29/fhdrvR3t4ONKG+iKUAYImBCLHLMlA0QlEUdHZ2Ynt7mz8WiUTQ0dFhOwPVWWpW+xKJBDo7O3nWK72ZmRlHw/xwOIxCocCndo3WF/X29uLo6MhwqTwhdjgKFIODg44ONH32qdnZWQwMDODp06eW33rnqdH2+f1+DAwMIJPJmH44ZRKJBM+O5fF4eNpAu2T19fcWCYVCWFhYoOtoiGO2A0UikUAwGMTm5qajA62jowPLy8solUoYHx/H33//famuDm2kfcPDw0BlylKvpaUlfg5hc3MTDx48cHSORFZffx4jlUohmUxS8l/imK1AwX4ZyGazjobCLS0tiEajSKVS/Gy9PvHuRWukfYqioKenB6urq44CpxV2joTl6HRKVp+S/5J6SQOFqqqIRqNVP83JaJqGQqFgyLKNSr6JXC7XtA9XvRptn6IoaG1t5fk+WWLd/v5+pFIpR6MC0f7+vviQI7L6pVJJfIgQS5aBQlEUxONx7O3tWQYJlok7mUwaHt/e3kYwGORD3UQiAZ/Ph62tLcPzLord9tXaP/2Qn/3t7u4im81W5f60a3h4GG63u+7zHbL6ZvtHiIzlgiv9Yis98W5f7Hm1Rh36BU3igqDLwE77rPZPb3Z2lucLRY3FXAzrPzGnppi0p9n161kwRwhkgYIQQiCbehBCCChQEELsoEBBCJGiQEEIkaJAQQiRokBBCJGiQEEIkaJAQQiRokBBCJGyXJkpy8loh7iM+LLlbGykfbL+EctRYxm1uAxbXB4va5+snGHPE7dPiB3/3dbW9j/ig8yLFy/w7bffYmVlBSsrK3j//ffxySef4Oeff8Zff/0lPr2Kqqr47LPPsLy8jK+++goulwvRaBSvX7/GixcvxKefu0bbJ+ufnp4efPDBB3jw4AEePXqElZUVfPfdd/zqzmQyiUAggLt37+Kbb77B8+fPcevWLXz44Yf48ccfpe2TlTOKomB4eBjHx8d4/fo1vv/+e91eECLnaOohy8koalZOyrPS7PY57R+v14tyucwv4jo4OMDR0REvl7VPVs5Eo1EUCgU8f/7c8DghdjkKFLKcjHrNykl5Vs6ifU76B5VkxT6fD7Ozs4DuMvH19XVp+27dumVZztrPMoRvbGzw5xHilDRQJCQ5Ga00mpPyrDWjfbL+cblcmJqa4s9huS9QyTh1584deDwepNNpBAIBfPnllzzQWLXv6tWrluWs/SxDuJPzSoSIpIFClpNRppGclOeh0fZZ9Y8+XyW7r8bQ0BAPFoqi4Ouvv0a5XMbExAQAVPWvWfvYOSKz8oODAyQsMoQT4oQ0UOjJcjKKGslJeR6a3T5Z/2QyGRSLRf4aIyMjODo6wvz8PPL5PO7fv284xyBrn1V5V1dXwxnCCWEcBQpGlpMRTchJedbOsn1m/dPe3g632w1UUv17PB7Dycx8Po9yuQzYaN/Tp08ty69fv462tjYMDQ3xaU9/fz+CwWDVFIgQGUeBwiwnY62cknCQk/Ki2G2f2f6JzPqHGRkZAQCsr68jn88jl8uhs7OT3+JPVVX4fD6e/FbWPqtycdoTi8WQzWaxu7uLWCxWc60FIWYsF1yJi4HEnIyMVU5J/YIgcUHSZWCnfWb7J+sf/bZRYzEVauQlFV9D1j5ZuV4ymYTX661qAyEyloGCEELgdOpBCPl3okBBCJGiQEEIkaJAQQiRokBBCJGiQEEIkaJAQQiRokBBCJGyteCKrUAUVw3aIa5ONEvVdlEabZ9+ZaWY5o6R9V+95WKqPXFlpqIomJychMvl4nWc7h8hkI0o/H4/FhYWcPXqVRwfH4vFUqqqIhKJYHFxkV9mHY1GHV2mfpYabR+7xoJdS1EoFDA+Ps6Txsj6r9Fy8XoOTdMwNDTEX1/TNIyOjhouc49EIvzaEkLssgwUw8PD2NzcxMrKilhki91UbRel0fYtLS0ZrpvY2NiA2+1GX18fYKP/Gi0XyVLxsSQ33d3dYhEhliwDxczMTN3DVFkqt3pSzTXTebRP1n+Nlotkqfh6e3txdHSEZ8+eiUWEWLIMFI2yk6rtIjW7fYODg+f+QUxIUvGpqopUKoV0Oo1QKISFhYWqq38JkTnTQAFJqrbLoFntSyQSCAaD2NzcPNcPolUqPgjnMVKpFJLJJD+3QohdZxoorFK1XQbNah/75SSbzTqaKjSbLBXf2toa9vb2EA6HxSJCLJ1ZoJClcjvPb91amtU+VVURjUarfrq8SGap+BiWQYsQu5oSKMxSxVmlarsM7LbPbP8URUE8Hsfe3t6lCBKyVHxm+0eIjOWCK3ExEiOmdDNLFQeHqdougp32me2fmMaOYf0j679Gy52m4jNbEEaIjGWgIIQQNGvqQQh5u1GgIIRIUaAghEhRoCCESFGgIIRIUaAghEhRoCCESFGgIIRIUaAghEjZXplplrdRRlxGfNlyNjbaPjv1a/Wd3+/H3bt30dHRYXguKku05+fnLcvZEnpZzk477SNExtaIQlVVhEKhmnkbrTSak/KsNdo+WX2rnJf5fB7j4+M8l0QsFsPExASKxSJKpZK0HDZydiYSCUSjUd6+xcVFRCIR2/tHCGMrUAwODmJnZweFQkEsstRoTsqz1mj7ZPWd5ryMRCJwu91YX18Xi4Aa5bKcneFw2HAZ/draGgqFgu39I4SRBopEIoHOzk7Tg9fMeeSkbESj7bNT30nOS7/fj4GBAdOcl7JyM2LuiVKpZGv/CNGzDBTs4MxkMo4OTqbZOSmbrdH2NVpfr6+vD263GxsbG2IRYKMcNXJ2lkolhEIhnp5fURSEQiGhFiFyloFieHgYqAyp69WsnJRnpdH2NVqfEacxIll5okbOzpmZGRQKBUxNTSGdTmNyctJ25i5C9EwDhaIo6Onpwerqat0HV7NyUp6VRtvXaH1GVVV0dnaajhZk5eyXjVo5O2/fvs1Pdo6OjuLKlSsol8t1v6fk38kyULS2tmJsbIyngw8Gg+jv70cqlZKeOW9WTsqz0mj7Gq2vJ04ZRFblTnJ21jqvQogdpoFCnwae/e3u7iKbzSIejxs+HGY5Je3mpLwodtt3lvvHPrxmaf6typ3k7GT3IS0UClWjDkJkbC+4QuUDUyqVqg5Ks5ySsJmT8iLZaV+9+ycudmLEBVMej8eQ61LPqlyWs1MVbmJMi61IvRwFCkLIv5Pp1IMQQhgKFIQQKQoUhBApChSEECkKFIQQKQoUhBApChSEECkKFIQQKQoUhBAp05WZLKdjW1ubYVlyMpmE1+s1ZFYiF4stFb/sS7TZMZXL5aqWwr/pWF5U6Jbyv3z5EpOTk3C5XABQ8xKA8yDmZ9VfQsCwa4EAVOVdhZ0RxfHxMXp7e8WHyTlRVRXLy8vSq3XfVIqiYHl5mV9Y9yZKJBJQFIXnJmUXTWqahtHRUZ7r9CzI+k9VVTx48AC5XI5f3CkGCQAYGRkBKkGuFmmg2N/fRyAQ4FmSyOXDrvS9zKOJt1lXVxcODw9rpgG4aIODg9A0zXIko6oqrl27ZpmgSjr1yOVy8Hq92N7extLSUtXUQ7xCkg1rFEXBF198gV9//RV9fX0oFov4/fff0dfXZxj6iPVrDZ/ZsK5WmRXW1lKpxIeFxWKRX4mpqipisRjS6TTi8ThcLldVynv9kBLC8FG2fVl9q9fv7u42XPmpx7Zh5+pQsX/Fvh8YGICmabh16xZQo/2NEtuISvvX19cNw3I9/e0KyuWy4RswkUggEong4cOHUBSlqv3i+ye+fq3hP+ujWmV2JJNJBAIB036zmnJZvT+o0X72/rS3t1v2n/4zmE6nq66IZvRt29raQiwWw6NHj5xPPVDJ7tzT0yM+DEVRcPPmTT6kWVxchM/n48OglpYWvPfee1hZWUFbWxveffddrK6uorOzE4qiIFEjnXw0GjUdRtUjGAzC6/UiFothenoabrebp/gDAJfLhbGxMWQyGcQqKe/ZMCyZTCIUCmF6eprXD4VChrwUVtu3U9/s9dfW1hCPx7G4uIjj42PeR7FYjB9s7DnT09NVtwNA5SDU9+/ExAQ8Hg9mZ2f5czo6OqAoCiYmJqra3yh2kK+vr/PXZ0NwNixnbV9dXeX7d/v2beTzeWxubvJjBbocrjs7O/xA1rd/YmICR0dH/P0TX79W/9eL3YohnU6jv78fHR0dmJubQzqdNvSvFdn74/f78fHHH+POnTtVx5es/wCgu7sbANDT08OTT4lJpyKRCADgyZMn/LFabAWKZ8+ewePxVM2TNU3DvXv3DM87PDw0pIPb3NxEuVzG6ekpNjY2UCwW+TwoHA5D0zRDOvm9vT2Ew2FeH5Xcj7E6h9bFYhHz8/NApb07Ozvwer2G5+i/iVmW6v7+fgQCAcNByeoHAgGexdps+36/31Z9s9dvRpZsMV1/rQ/f8fExFhYWkM/noVWydon9U6/e3l7s7e3V9b6h0l+ofCFBl2BYnxhI3362f6z/xNc36382dRO/7a3kdfddyWazKBaLmJiYMHxQZWTvTz6fx7179/gopZ73x+Vyobu7mwcRTdMQj8ehKAoURUEkEqmZFElkK1Dk83n89ttvNU9qsuxP6XQac3NzNe9sVcv169f5B5LVZ+n2zpr+g6jPpI1KUBofH8fp6Sncbjf29/d1Nf//nI0sy7bH48FHH31kq77Z68veOBm/3w+Px1OVrp+9Fvu2OUtODuha2AebfXH09vZaJhhmWP96vV4Eg0HD8aWfBl4ku+9PMpls6PNxfHyMx48f8//ZbTcURUE0GrWd8cxWoEDlBbxeL9555x3+WDKZRGdnJx9aOzm7+8cff6BcLiObzfJoJw6dzoLX67WVXPbg4ABHR0dVyXK7uross2yz7f/yyy911W+WfD6Pcrlc9WFlgfzly5eGxy+rra0tXLt2DZ9++ilu3LhhmmCY0fdvqVTC7u5u1fHVjEDcKDvvj/hrSqySjtIu9h6LXwqnp6f4559/EAgEDIF0bGwMra2tmJqaqpo+2Q4Umqbh5OTE8E3q9XoNB/3w8LDtEQUqOScVRama0ohYVG303IWqqvD5fLaSy+bzeeRyOcN9MVRVhaIopkM1/fbrqV8Le7NrjeZktre34fP5eP+yoaZ+OmQH63+nc/vt7W0+jBZ/y2dYQBanm8za2hpevXqFzz//HCcnJ5ajCaVy3xKW3Hhra8twzsxMIpGoa/8aJXt/urq6cHp6yo+BROWWDHpW/adpGl69emW4M1w0GsXJyQl++OGHqltWLi4u4s8//8T09HTVl7XtQAEAP/30E65fv87/f/z4MdxuNz+J4/V6HUW8paUlrK+vGzJ9NyMg6OlPMo2OjmJ5ednWUAs17osxNjaG9fV1Q32r7dupL6NpGjKZjGGKxg5o9gGempqCy+XC0NCQoXxpaQmapvH+nZqaws7OjqO5OCrf6qenp4agZ8fS0hLf/7m5OeRyuarjI5/PY3V1FT6fj++f+G22vb2N1tbWmgHe5XLx/p2amkImkzGc7F1eXkY0GjUcX+cVEFgAYlNy9h6y/ZO9P0+ePMHR0RHfv4GBgaqfYGX9Nz8/D4/Hw8usfp2xYvrz6NtA9rNVo856+5cFGw0AuJB91f8kqh8JmT1Oms/RiIL8O7F7qjq9X0kzsOF4vbe1JM1BgYKYYmsF2HUkTqcsjVBVFalUig/HnUzXSPP9H4A8VsZre2JFAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "c2d1651f",
   "metadata": {},
   "source": [
    "Now, from the EDA we know that we are dealing with a highly imbalanced dataset. To tackle this, we will implement a weighted cross-entropy loss function that assigns higher weights to the minority classes. This approach will help the model pay more attention to underrepresented classes during training. We create training and test sets and use ADAM optimizer for training the CNN model.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cec5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Load dates and calculate stats (fast)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_meta = \u001b[43mpd\u001b[49m.read_parquet(\u001b[33m'\u001b[39m\u001b[33m../data/cleaned_data.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m train_dates = df_meta[df_meta[\u001b[33m'\u001b[39m\u001b[33mset\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdatum\u001b[39m\u001b[33m'\u001b[39m].unique().astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m      4\u001b[39m val_dates = df_meta[df_meta[\u001b[33m'\u001b[39m\u001b[33mset\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdatum\u001b[39m\u001b[33m'\u001b[39m].unique().astype(\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Load dates and calculate stats (fast)\n",
    "df_meta = pd.read_parquet('../data/cleaned_data.parquet')\n",
    "train_dates = df_meta[df_meta['set'] == 'train']['datum'].unique().astype(str)\n",
    "val_dates = df_meta[df_meta['set'] == 'test']['datum'].unique().astype(str)\n",
    "freq_dangerLevel = df_meta['danger_level'].value_counts(normalize=True)\n",
    "print(\"Class Distribution in Training Data: {freq_dangerLevel}\")\n",
    "\n",
    "# Calculate stats\n",
    "stats = {col: {'mean': df_meta[col].mean(), 'std': df_meta[col].std()} for col in FEATURE_NAMES}\n",
    "\n",
    "# 2. Instantiate Datasets\n",
    "train_ds = AvalancheDataset(train_dates, DYNAMIC_DIR, TARGET_DIR, STATIC_FILE, stats, FEATURE_NAMES)\n",
    "val_ds = AvalancheDataset(val_dates, DYNAMIC_DIR, TARGET_DIR, STATIC_FILE, stats, FEATURE_NAMES)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 3. Initialize Model\n",
    "# Input channels = 11 dynamic + 4 static = 15\n",
    "model = AvalancheNet(in_channels=len(FEATURE_NAMES)+4, num_classes=5).to(DEVICE)\n",
    "\n",
    "# 4. Loss Function with Class Weights\n",
    "class_counts = torch.tensor([\n",
    "    0.0,      # Class 0 (NoData) - will be ignored anyway\n",
    "    0.211133, # Class 1\n",
    "    0.413133, # Class 2\n",
    "    0.358004, # Class 3\n",
    "    0.017729  # Class 4 \n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Inverse frequency weighting: weight = 1 / frequency\n",
    "# For class 0, set weight to 0 since we ignore it\n",
    "class_weights = torch.zeros(5, dtype=torch.float32)\n",
    "class_weights[1:] = 1.0 / class_counts[1:]  # Only weight classes 1-4\n",
    "class_weights = class_weights / class_weights[1:].sum()  # Normalize\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE), ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X, Y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)  # Output: (Batch, 5, H, W)\n",
    "        \n",
    "        loss = criterion(outputs, Y) # Y is (Batch, H, W)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Optional: Add Validation Loop here\n",
    "    # Save Checkpoint\n",
    "    torch.save(model.state_dict(), f\"models/cnn_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1559a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU device count: 1\n",
      "GPU name: NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "GPU memory: 8.55 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabio\\miniconda3\\envs\\avalanche_project\\Lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5050 Laptop GPU with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5050 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected! Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc65e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0.dev20250310+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5050 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabio\\miniconda3\\envs\\avalanche_project\\Lib\\site-packages\\torch\\cuda\\__init__.py:287: UserWarning: \n",
      "NVIDIA GeForce RTX 5050 Laptop GPU with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5050 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avalanche_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
