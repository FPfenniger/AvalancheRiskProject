{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc79b42e",
   "metadata": {},
   "source": [
    "# **Avalanche Risk Project**\n",
    "\n",
    "Advanced Data Analytics, Fall 2025\n",
    "\n",
    "The following project examines the feasibility of deep learning models (3D-CNN, convLSTM) compared to traditional, state-of-the-art methods for predicting avalanche danger levels. Using a comprehensive dataset from the Swiss Federal Institute for Snow and Avalanche Research (SLF) spanning from 1997-2020, the project heavily relies on the work of P√©rez-Guill√©n et al. (2022) and Maissen et al. (2024) and adopts many of their preprocessing and modeling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdmfrom sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Kaggle\n",
    "is_kaggle = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "if is_kaggle:\n",
    "    # Kaggle Input Paths (Read-Only)\n",
    "    data_dir = '/kaggle/input/avalanche-data/data' \n",
    "    grid_dir = os.path.join(data_dir, 'grids')\n",
    "    \n",
    "    # Kaggle Output Paths (Writable)\n",
    "    output_dir = '/kaggle/working'\n",
    "    model_save_dir = os.path.join(output_dir, 'models')\n",
    "    \n",
    "    print(\"Running on Kaggle. Paths adjusted.\")\n",
    "\n",
    "else:\n",
    "    # Local Paths\n",
    "    data_dir = '../data'\n",
    "    GRID_DIR = os.path.join(data_dir, 'grids')\n",
    "    \n",
    "    output_dir= 'results'\n",
    "    model_save_dir = 'models'\n",
    "    \n",
    "    print(\"Running Locally. Standard paths used.\")\n",
    "\n",
    "# Create output directories if they don't exist (critical for Kaggle)\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_kaggle:\n",
    "    print(\"Installing missing dependencies for Kaggle: \")\n",
    "    !pip install -q xarray-spatial rasterio geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b76784",
   "metadata": {},
   "source": [
    "## **5. CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a61502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic features \n",
    "dynamic_features = [\n",
    "    'delta_elevation',   \n",
    "    'Pen_depth',          \n",
    "    'HN24',              # Daily snowfall (24h)\n",
    "    'MS_Snow',           \n",
    "    'TA',                # Air temperature\n",
    "    'wind_trans24',      # 24h wind transport\n",
    "    'RH',                # Relative humidity\n",
    "    'min_ccl_pen',       # Critical crack length\n",
    "    'relative_load_3d',  # 3-day relative load\n",
    "    'wind_u',            # Zonal wind component\n",
    "    'wind_v'             # Meridional wind component\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce0ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('../data/cleaned_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a717339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3124\n",
      "  Date range: 1997-11-12 to 2016-05-08\n",
      "\n",
      "Validation samples (2016/17 + 2017/18 winters): 336\n",
      "  Date range: 2016-11-09 to 2018-04-14\n",
      "\n",
      "Test samples (held out): 359\n",
      "  Date range: 2019-11-16 to 2019-04-24\n",
      "\n",
      "‚úÖ Temporal split verified: train ‚Üí val (2 winters) ‚Üí test\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED: Use last 2 complete winter seasons for validation\n",
    "train_full = df[df['set'] == 'train']['datum'].unique()\n",
    "train_full_sorted = pd.to_datetime(np.sort(train_full))\n",
    "\n",
    "# Define validation winters: 2016/17 and 2017/18 (Nov-May)\n",
    "val_start = pd.Timestamp('2016-10-01')\n",
    "val_end = pd.Timestamp('2018-06-30')\n",
    "\n",
    "# Split based on dates\n",
    "train_dates = train_full_sorted[train_full_sorted < val_start].astype(str)\n",
    "val_dates = train_full_sorted[(train_full_sorted >= val_start) & \n",
    "                               (train_full_sorted <= val_end)].astype(str)\n",
    "\n",
    "test_dates = df[df['set'] == 'test']['datum'].unique().astype(str)\n",
    "\n",
    "print(f\"Training samples: {len(train_dates)}\")\n",
    "print(f\"  Date range: {train_dates[0]} to {train_dates[-1]}\")\n",
    "print(f\"\\nValidation samples (2016/17 + 2017/18 winters): {len(val_dates)}\")\n",
    "print(f\"  Date range: {val_dates[0]} to {val_dates[-1]}\")\n",
    "print(f\"\\nTest samples (held out): {len(test_dates)}\")\n",
    "print(f\"  Date range: {test_dates[0]} to {test_dates[-1]}\")\n",
    "\n",
    "# Verify no temporal overlap\n",
    "assert pd.to_datetime(val_dates[0]) > pd.to_datetime(train_dates[-1]), \"‚ùå Temporal leakage!\"\n",
    "assert pd.to_datetime(test_dates[0]) > pd.to_datetime(val_dates[-1]), \"‚ùå Test leakage!\"\n",
    "print(\"\\n‚úÖ Temporal split verified: train ‚Üí val (2 winters) ‚Üí test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "grid_dir = '../data/grids'\n",
    "dynamic_dir = os.path.join(grid_dir, 'dynamic')\n",
    "target_dir = os.path.join(grid_dir, 'targets')\n",
    "static_file = os.path.join(grid_dir, 'static_terrain.npy')\n",
    "\n",
    "# Hyperparameters - Optimized for Kaggle T4 GPU (16GB VRAM)\n",
    "batch_size = 4        # Kaggle T4 can handle 4-8, start with 4\n",
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "lookback = 7          # T-6 to T\n",
    "num_classes = 5       # 0=NoData (Ignore), 1, 2, 3, 4 (High)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Running on device: {device}\")\n",
    "print(f\"Batch size: {batch_size} (4x faster than batch_size=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvalancheDataset(Dataset):\n",
    "    def __init__(self, date_list, feature_dir, target_dir, static_file, stats, features, lookback=7):\n",
    "        self.dates = date_list\n",
    "        self.feature_dir = feature_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.stats = stats\n",
    "        self.features = features\n",
    "        self.lookback = lookback\n",
    "        \n",
    "        # Load static data once (H, W, 4) -> (Elevation, Slope, AspectSin, AspectCos)\n",
    "        self.static_data = np.load(static_file).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_date = pd.to_datetime(self.dates[idx])\n",
    "        \n",
    "        # 1. Build Dynamic Sequence (T-6 to T) \n",
    "        frames = []\n",
    "        start_date = target_date - pd.Timedelta(days=self.lookback - 1)\n",
    "        \n",
    "        for i in range(self.lookback):\n",
    "            d_str = (start_date + pd.Timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "            f_path = os.path.join(self.feature_dir, f\"{d_str}.npz\")\n",
    "            \n",
    "            if os.path.exists(f_path):\n",
    "                # Load raw data (H, W, C)\n",
    "                raw = np.load(f_path)['data']\n",
    "                norm = np.zeros_like(raw)\n",
    "                # Normalize channel-wise\n",
    "                for c, name in enumerate(self.features):\n",
    "                    mu, std = self.stats[name]['mean'], self.stats[name]['std']\n",
    "                    norm[..., c] = (raw[..., c] - mu) / (std + 1e-6)\n",
    "                frames.append(norm)\n",
    "            else:\n",
    "                # Missing day? Fill with zeros\n",
    "                frames.append(np.zeros_like(self.static_data[..., :len(self.features)]))\n",
    "        \n",
    "        # Stack -> (Time, H, W, C_dyn)\n",
    "        dynamic_tensor = np.stack(frames, axis=0)\n",
    "\n",
    "        # --- 2. Add Static Data ---\n",
    "        # Repeat static data for every timestep\n",
    "        static_expanded = np.tile(self.static_data[np.newaxis, ...], (self.lookback, 1, 1, 1))\n",
    "        \n",
    "        # Combine -> (Time, H, W, Total_Channels)\n",
    "        full_cube = np.concatenate([dynamic_tensor, static_expanded], axis=-1)\n",
    "\n",
    "        # --- 3. Load Target ---\n",
    "        t_path = os.path.join(self.target_dir, f\"{self.dates[idx]}.npy\")\n",
    "        if os.path.exists(t_path):\n",
    "            # Background is -1, Classes are 1,2,3,4.\n",
    "            # We shift classes to 0,1,2,3,4 for PyTorch? \n",
    "            # Actually, simpler: Let's map -1 -> 0 (Ignore), 1->1, etc.\n",
    "            label = np.load(t_path).astype(np.int64)\n",
    "            label[label == -1] = 0 # Move \"No Data\" to class 0\n",
    "        else:\n",
    "            label = np.zeros((self.static_data.shape[0], self.static_data.shape[1]), dtype=np.int64)\n",
    "\n",
    "        # --- 4. Permute for PyTorch ---\n",
    "        # Input: (Channels, Time, Height, Width)\n",
    "        X = torch.from_numpy(full_cube).permute(3, 0, 1, 2)\n",
    "        Y = torch.from_numpy(label)\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvalancheNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AvalancheNet, self).__init__()\n",
    "        \n",
    "        # 3D Convolutions: Process (Time, Height, Width)\n",
    "        # padding='same' keeps the output map size equal to input size\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        # Collapse Time Dimension: Average over the 7 days\n",
    "        # Output becomes (Batch, 64, H, W)\n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        \n",
    "        # Final 2D Convolutions to map to classes\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, C, T, H, W)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Aggregate Time: (Batch, 64, 1, H, W) -> Squeeze -> (Batch, 64, H, W)\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        \n",
    "        # Predict Classes\n",
    "        x = self.final_conv(x) # (Batch, Num_Classes, H, W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b93399",
   "metadata": {},
   "source": [
    "## Architecture Variants for Hyperparameter Tuning\n",
    "\n",
    "We'll test 4 different architectures to find the optimal model complexity:\n",
    "1. **Light**: Baseline 2-layer model (32‚Üí64 filters)\n",
    "2. **Medium**: Current model with dropout regularization\n",
    "3. **Deep**: 3-layer model (64‚Üí128‚Üí128 filters)\n",
    "4. **MaxPool**: Uses max pooling instead of average pooling for temporal aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f658562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvalancheNet_Light(nn.Module):\n",
    "    \"\"\"Lightweight baseline - 2 conv layers, 32‚Üí64 filters\"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AvalancheNet_Light, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AvalancheNet_Medium(nn.Module):\n",
    "    \"\"\"Medium model with dropout regularization - 32‚Üí64 filters + dropout\"\"\"\n",
    "    def __init__(self, in_channels, num_classes, dropout_rate=0.3):\n",
    "        super(AvalancheNet_Medium, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout3d(p=dropout_rate)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.dropout2 = nn.Dropout3d(p=dropout_rate)\n",
    "        \n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AvalancheNet_Deep(nn.Module):\n",
    "    \"\"\"Deeper model - 3 conv layers, 64‚Üí128‚Üí128 filters\"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AvalancheNet_Deep, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(128, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn3 = nn.BatchNorm3d(128)\n",
    "        \n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        self.final_conv = nn.Conv2d(128, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AvalancheNet_MaxPool(nn.Module):\n",
    "    \"\"\"Uses MaxPooling instead of AvgPooling - captures extreme events\"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AvalancheNet_MaxPool, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        # Key difference: MaxPooling captures peak conditions over 7 days\n",
    "        self.pool_time = nn.MaxPool3d((7, 1, 1))\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fedafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, dataloader, device, criterion):\n",
    "    \"\"\"\n",
    "    Calculate validation metrics including Macro-F1 score.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains loss, macro_f1, and per-class F1 scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions (argmax over class dimension)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Flatten spatial dimensions and filter out ignore_index (0)\n",
    "            preds_flat = preds.cpu().numpy().flatten()\n",
    "            targets_flat = y.cpu().numpy().flatten()\n",
    "            \n",
    "            # Only keep non-zero labels (ignore background)\n",
    "            mask = targets_flat != 0\n",
    "            preds_flat = preds_flat[mask]\n",
    "            targets_flat = targets_flat[mask]\n",
    "            \n",
    "            all_preds.extend(preds_flat)\n",
    "            all_targets.extend(targets_flat)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Calculate Macro-F1 (average F1 across classes 1-4)\n",
    "    macro_f1 = f1_score(all_targets, all_preds, average='macro', labels=[1, 2, 3, 4], zero_division=0)\n",
    "    \n",
    "    # Per-class F1 scores\n",
    "    per_class_f1 = f1_score(all_targets, all_preds, average=None, labels=[1, 2, 3, 4], zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'macro_f1': macro_f1,\n",
    "        'f1_class_1': per_class_f1[0],\n",
    "        'f1_class_2': per_class_f1[1],\n",
    "        'f1_class_3': per_class_f1[2],\n",
    "        'f1_class_4': per_class_f1[3]\n",
    "    }"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c2d1651f",
   "metadata": {},
   "source": [
    "Now, from the EDA we know that we are dealing with a highly imbalanced dataset. To tackle this, we will implement a weighted cross-entropy loss function that assigns higher weights to the minority classes. This approach will help the model pay more attention to underrepresented classes during training. We create training and test sets and use ADAM optimizer for training the CNN model.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31584074",
   "metadata": {},
   "source": [
    "### Model Architecture Summary\n",
    "\n",
    "**Input Tensor Shape**: `(Batch=4, Channels=15, Time=7, Height=211, Width=470)`\n",
    "- **11 dynamic features** (weather/snow conditions over 7 days)\n",
    "- **4 static features** (terrain: elevation, slope, aspect_sin, aspect_cos)\n",
    "\n",
    "**3D CNN Processing**:\n",
    "1. Conv3D layer 1: Extracts spatio-temporal patterns ‚Üí 32 filters\n",
    "2. Conv3D layer 2: Deeper features ‚Üí 64 filters  \n",
    "3. Temporal pooling: Aggregates 7 days into single time step\n",
    "4. Conv2D output: Maps to 5 danger level classes per pixel\n",
    "\n",
    "**Output**: `(Batch=4, Classes=5, Height=211, Width=470)` - Danger level prediction for each grid cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb6bc8",
   "metadata": {},
   "source": [
    "## 2-3 Week Training Plan\n",
    "\n",
    "**Week 1: Architecture Search (4 runs √ó 5 epochs ‚âà 8-12 hours)**\n",
    "- Test Light, Medium, Deep, MaxPool variants\n",
    "- Identify best architecture based on validation loss\n",
    "\n",
    "**Week 2: Hyperparameter Tuning (6 runs √ó 10 epochs ‚âà 20-25 hours)**\n",
    "- Test 3 learning rates on best architecture\n",
    "- Test 3 dropout rates for regularization\n",
    "\n",
    "**Week 3: Final Training (2 runs √ó 20 epochs ‚âà 12-16 hours)**\n",
    "- Train best model with optimal hyperparameters\n",
    "- Full 20-30 epoch training for final evaluation\n",
    "\n",
    "**Total: ~40-50 GPU hours on Kaggle (well within 30 hours/week limit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761a529",
   "metadata": {},
   "source": [
    "## ‚ö° Speed Optimization for Kaggle\n",
    "\n",
    "**Updated for Kaggle T4 GPU (16GB VRAM):**\n",
    "- **Batch size**: Increased from 1 ‚Üí 4 (**4x faster!**)\n",
    "- **Epochs reduced** for initial screening (3 for Week 1, 6 for Week 2)\n",
    "- **Week 3**: Still 25 epochs for final model\n",
    "\n",
    "### Realistic Timeline on Kaggle:\n",
    "\n",
    "| Week | Task | GPU Hours | Speed |\n",
    "|------|------|-----------|-------|\n",
    "| **Week 1** | 4 archs √ó 3 epochs | **~6-8 hours** | 4x faster |\n",
    "| **Week 2** | 3 LRs √ó 6 epochs | **~9-12 hours** | 4x faster |\n",
    "| **Week 3** | Final √ó 20 epochs | **~10-12 hours** | 4x faster |\n",
    "| **TOTAL** | | **~25-32 hours** ‚úÖ | Fits in limit! |\n",
    "\n",
    "**Memory Check**: T4 has 16GB VRAM, your tensors at batch_size=4:\n",
    "- Input: `(4, 15, 7, 211, 470) √ó 4 bytes = 1.5 GB`\n",
    "- Activations: ~4-6 GB\n",
    "- **Total: ~8-10 GB** ‚úÖ (well under 16GB limit)\n",
    "\n",
    "### üö® Important: Validation vs Test\n",
    "\n",
    "**Currently, this notebook uses VALIDATION (test set from your data split):**\n",
    "```python\n",
    "val_dates = df[df['set'] == 'test']['datum'].unique()\n",
    "```\n",
    "\n",
    "**This is correct for hyperparameter tuning!** You should:\n",
    "1. ‚úÖ Tune on validation set (current approach)\n",
    "2. ‚ùå Never touch true test set until final evaluation\n",
    "3. üéØ After Week 3, evaluate final model on held-out test set once\n",
    "\n",
    "Your data has `set` column with values like `'train'` and `'test'`. The `'test'` portion is actually your **validation set** for tuning. You'll need a separate true test set for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cec5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Load dates and calculate stats (fast)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_meta = \u001b[43mpd\u001b[49m.read_parquet(\u001b[33m'\u001b[39m\u001b[33m../data/cleaned_data.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m train_dates = df_meta[df_meta[\u001b[33m'\u001b[39m\u001b[33mset\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdatum\u001b[39m\u001b[33m'\u001b[39m].unique().astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m      4\u001b[39m val_dates = df_meta[df_meta[\u001b[33m'\u001b[39m\u001b[33mset\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdatum\u001b[39m\u001b[33m'\u001b[39m].unique().astype(\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Print class distribution\n",
    "freq_dangerLevel = df['danger_level'].value_counts(normalize=True)\n",
    "print(f\"Class Distribution in Training Data:\\n{freq_dangerLevel}\\n\")\n",
    "\n",
    "# 2. Instantiate Datasets (using previously defined train_dates, val_dates, stats)\n",
    "train_ds = AvalancheDataset(train_dates, dynamic_dir, target_dir, static_file, stats, dynamic_features)\n",
    "val_ds = AvalancheDataset(val_dates, dynamic_dir, target_dir, static_file, stats, dynamic_features)\n",
    "\n",
    "# Use num_workers for faster data loading (Kaggle optimization)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "# 3. Initialize Model\n",
    "# Input channels = 11 dynamic + 4 static = 15\n",
    "model = AvalancheNet(in_channels=len(dynamic_features)+4, num_classes=5).to(device)\n",
    "\n",
    "# 4. Loss Function with Class Weights\n",
    "class_counts = torch.tensor([\n",
    "    0.0,      # Class 0 (NoData) - will be ignored anyway\n",
    "    0.211133, # Class 1\n",
    "    0.413133, # Class 2\n",
    "    0.358004, # Class 3\n",
    "    0.017729  # Class 4 \n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Inverse frequency weighting: weight = 1 / frequency\n",
    "# For class 0, set weight to 0 since we ignore it\n",
    "class_weights = torch.zeros(5, dtype=torch.float32)\n",
    "class_weights[1:] = 1.0 / class_counts[1:]  # Only weight classes 1-4\n",
    "class_weights = class_weights / class_weights[1:].sum()  # Normalize\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device), ignore_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e3c30",
   "metadata": {},
   "source": [
    "## WEEK 1: Architecture Search\n",
    "\n",
    "Run this cell to compare all 4 architectures with 5 epochs each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e72669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create experiment tracking directory\n",
    "Path(\"experiments\").mkdir(exist_ok=True)\n",
    "\n",
    "# Define architectures to test\n",
    "architectures = {\n",
    "    'light': AvalancheNet_Light,\n",
    "    'medium': AvalancheNet_Medium,\n",
    "    'deep': AvalancheNet_Deep,\n",
    "    'maxpool': AvalancheNet_MaxPool\n",
    "}\n",
    "\n",
    "# Configuration for architecture search (shorter epochs)\n",
    "search_config = {\n",
    "    'batch_size': 4,  # Kaggle T4: 16GB VRAM (4x faster than batch_size=1)\n",
    "    'learning_rate': 1e-4,\n",
    "    'epochs': 3,  # Reduced for faster comparison (was 5)\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ARCHITECTURE SEARCH - WEEK 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for arch_name, arch_class in architectures.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {arch_name.upper()}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = arch_class(\n",
    "        in_channels=len(dynamic_features)+4, \n",
    "        num_classes=5\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\\n\")\n",
    "    \n",
    "    # Setup optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=search_config['learning_rate'])\n",
    "    scaler = GradScaler()  # AMP for faster training\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_macro_f1s = []\n",
    "    \n",
    "    for epoch in range(search_config['epochs']):\n",
    "        # Training with Mixed Precision\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{search_config['epochs']}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "            \n",
    "            # Scaled backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation with Macro-F1\n",
    "        val_metrics = calculate_metrics(model, val_loader, device, criterion)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_macro_f1s.append(val_metrics['macro_f1'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f} | Val Loss={val_metrics['loss']:.4f} | Macro-F1={val_metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results[arch_name] = {\n",
    "        'total_params': total_params,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_macro_f1s': val_macro_f1s,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'final_macro_f1': val_macro_f1s[-1]\n",
    "    }\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    torch.save(\n",
    "        model.state_dict(), \n",
    "        f\"experiments/week1_{arch_name}_final.pth\"\n",
    "    )\n",
    "    \n",
    "    # Save config\n",
    "    with open(f\"experiments/week1_{arch_name}_config.json\", 'w') as f:\n",
    "        json.dump({\n",
    "            'architecture': arch_name,\n",
    "            'config': search_config,\n",
    "            'results': results[arch_name]\n",
    "        }, f, indent=2)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 1 RESULTS: ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for arch_name, metrics in results.items():\n",
    "    print(f\"\\n{arch_name.upper()}:\")\n",
    "    print(f\"  Parameters: {metrics['total_params']:,}\")\n",
    "    print(f\"  Final Train Loss: {metrics['final_train_loss']:.4f}\")\n",
    "    print(f\"  Final Val Loss: {metrics['final_val_loss']:.4f}\")\n",
    "    print(f\"  Final Macro-F1: {metrics['final_macro_f1']:.4f}\")\n",
    "\n",
    "# Find best architecture based on Macro-F1 (not loss!)\n",
    "best_arch = max(results.items(), key=lambda x: x[1]['final_macro_f1'])\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üèÜ BEST ARCHITECTURE: {best_arch[0].upper()}\")\n",
    "print(f\"   Validation Loss: {best_arch[1]['final_val_loss']:.4f}\")\n",
    "print(f\"   Macro-F1: {best_arch[1]['final_macro_f1']:.4f}\")\n",
    "print(f\"   Parameters: {best_arch[1]['total_params']:,}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save summary\n",
    "with open('experiments/week1_summary.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'all_results': results,\n",
    "        'best_architecture': best_arch[0],\n",
    "        'best_val_loss': best_arch[1]['final_val_loss']\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves for all architectures\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Training Loss Comparison\n",
    "for arch_name, metrics in results.items():\n",
    "    epochs_range = range(1, len(metrics['train_losses']) + 1)\n",
    "    ax1.plot(epochs_range, metrics['train_losses'], label=arch_name, marker='o')\n",
    "\n",
    "ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss Comparison\n",
    "for arch_name, metrics in results.items():\n",
    "    epochs_range = range(1, len(metrics['val_losses']) + 1)\n",
    "    ax2.plot(epochs_range, metrics['val_losses'], label=arch_name, marker='o')\n",
    "\n",
    "ax2.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Macro-F1 Comparison\n",
    "for arch_name, metrics in results.items():\n",
    "    epochs_range = range(1, len(metrics['val_macro_f1s']) + 1)\n",
    "    ax3.plot(epochs_range, metrics['val_macro_f1s'], label=arch_name, marker='s', linewidth=2)\n",
    "\n",
    "ax3.set_title('Validation Macro-F1 Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Macro-F1')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter vs Performance scatter plot\n",
    "arch_names = list(results.keys())\n",
    "params = [results[a]['total_params'] / 1e6 for a in arch_names]  # In millions\n",
    "macro_f1s = [results[a]['final_macro_f1'] for a in arch_names]\n",
    "\n",
    "scatter = ax4.scatter(params, macro_f1s, s=300, alpha=0.6, c=range(len(arch_names)), cmap='viridis')\n",
    "for i, name in enumerate(arch_names):\n",
    "    ax4.annotate(name.upper(), (params[i], macro_f1s[i]), \n",
    "                xytext=(10, 5), textcoords='offset points', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax4.set_xlabel('Model Size (Million Parameters)', fontsize=12)\n",
    "ax4.set_ylabel('Final Macro-F1', fontsize=12)\n",
    "ax4.set_title('Model Complexity vs Macro-F1', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiments/week1_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualizations saved to experiments/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c779d9",
   "metadata": {},
   "source": [
    "## WEEK 2: Hyperparameter Tuning\n",
    "\n",
    "**Instructions**: \n",
    "1. Load the best architecture from Week 1 results\n",
    "2. Test different learning rates and dropout values\n",
    "3. Run this after reviewing Week 1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b35b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Week 1 results to determine best architecture\n",
    "with open('experiments/week1_summary.json', 'r') as f:\n",
    "    week1_summary = json.load(f)\n",
    "\n",
    "best_arch_name = week1_summary['best_architecture']\n",
    "print(f\"üìå Using best architecture from Week 1: {best_arch_name.upper()}\\n\")\n",
    "\n",
    "# Map architecture name to class\n",
    "arch_map = {\n",
    "    'light': AvalancheNet_Light,\n",
    "    'medium': AvalancheNet_Medium,\n",
    "    'deep': AvalancheNet_Deep,\n",
    "    'maxpool': AvalancheNet_MaxPool\n",
    "}\n",
    "BestArchitecture = arch_map[best_arch_name]\n",
    "\n",
    "# Hyperparameter grid\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "dropout_rates = [0.2, 0.3, 0.5] if best_arch_name == 'medium' else [None]\n",
    "\n",
    "# Configuration for hyperparameter tuning\n",
    "tuning_config = {\n",
    "    'batch_size': 4,  # 4x faster than batch_size=1\n",
    "    'epochs': 6,  # Reduced from 10 (still enough to see trends)\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "week2_results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - WEEK 2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test learning rates\n",
    "for lr in learning_rates:\n",
    "    run_name = f\"lr_{lr:.0e}\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing Learning Rate: {lr}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    model = BestArchitecture(\n",
    "        in_channels=len(dynamic_features)+4,\n",
    "        num_classes=5\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_macro_f1s = []\n",
    "    \n",
    "    for epoch in range(tuning_config['epochs']):\n",
    "        # Training with AMP\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation with Macro-F1\n",
    "        val_metrics = calculate_metrics(model, val_loader, device, criterion)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_macro_f1s.append(val_metrics['macro_f1'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train={avg_train_loss:.4f} | Val={val_metrics['loss']:.4f} | Macro-F1={val_metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    week2_results[run_name] = {\n",
    "        'lr': lr,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_macro_f1s': val_macro_f1s,\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'final_macro_f1': val_macro_f1s[-1]\n",
    "    }\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"experiments/week2_{run_name}.pth\")\n",
    "    \n",
    "    with open(f\"experiments/week2_{run_name}_config.json\", 'w') as f:\n",
    "        json.dump({\n",
    "            'architecture': best_arch_name,\n",
    "            'learning_rate': lr,\n",
    "            'config': tuning_config,\n",
    "            'results': week2_results[run_name]\n",
    "        }, f, indent=2)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 2 RESULTS: LEARNING RATE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for run_name, metrics in week2_results.items():\n",
    "    print(f\"\\n{run_name}:\")\n",
    "    print(f\"  Learning Rate: {metrics['lr']}\")\n",
    "    print(f\"  Final Val Loss: {metrics['final_val_loss']:.4f}\")\n",
    "    print(f\"  Final Macro-F1: {metrics['final_macro_f1']:.4f}\")\n",
    "\n",
    "best_lr_run = max(week2_results.items(), key=lambda x: x[1]['final_macro_f1'])\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üèÜ BEST LEARNING RATE: {best_lr_run[1]['lr']}\")\n",
    "print(f\"   Validation Loss: {best_lr_run[1]['final_val_loss']:.4f}\")\n",
    "print(f\"   Macro-F1: {best_lr_run[1]['final_macro_f1']:.4f}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save Week 2 summary\n",
    "with open('experiments/week2_summary.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'architecture': best_arch_name,\n",
    "        'all_results': week2_results,\n",
    "        'best_lr': best_lr_run[1]['lr'],\n",
    "        'best_val_loss': best_lr_run[1]['final_val_loss']\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd36075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Week 2 results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation Loss Comparison\n",
    "for run_name, metrics in week2_results.items():\n",
    "    epochs_range = range(1, len(metrics['val_losses']) + 1)\n",
    "    ax1.plot(epochs_range, metrics['val_losses'], \n",
    "            label=f\"LR={metrics['lr']:.0e}\", marker='o', linewidth=2)\n",
    "\n",
    "ax1.set_title('Learning Rate Comparison - Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Macro-F1 Comparison\n",
    "for run_name, metrics in week2_results.items():\n",
    "    epochs_range = range(1, len(metrics['val_macro_f1s']) + 1)\n",
    "    ax2.plot(epochs_range, metrics['val_macro_f1s'], \n",
    "            label=f\"LR={metrics['lr']:.0e}\", marker='s', linewidth=2)\n",
    "\n",
    "ax2.set_title('Learning Rate Comparison - Macro-F1', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Macro-F1', fontsize=12)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiments/week2_lr_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Week 2 visualization saved to experiments/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719ea5b",
   "metadata": {},
   "source": [
    "## WEEK 3: Final Training\n",
    "\n",
    "**Instructions**: \n",
    "1. Load best architecture and hyperparameters from Week 1-2\n",
    "2. Train for 20-30 epochs with early stopping\n",
    "3. Save final model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best configuration from Week 1 and Week 2\n",
    "with open('experiments/week1_summary.json', 'r') as f:\n",
    "    week1_summary = json.load(f)\n",
    "\n",
    "with open('experiments/week2_summary.json', 'r') as f:\n",
    "    week2_summary = json.load(f)\n",
    "\n",
    "best_arch_name = week1_summary['best_architecture']\n",
    "best_lr = week2_summary['best_lr']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL TRAINING - WEEK 3\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Architecture: {best_arch_name.upper()}\")\n",
    "print(f\"Learning Rate: {best_lr}\")\n",
    "print(f\"Epochs: 25 (with early stopping)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Initialize final model\n",
    "BestArchitecture = arch_map[best_arch_name]\n",
    "final_model = BestArchitecture(\n",
    "    in_channels=len(dynamic_features)+4,\n",
    "    num_classes=5\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Learning rate scheduler (reduces LR when Macro-F1 plateaus)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Early stopping based on Macro-F1\n",
    "best_macro_f1 = 0.0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "final_train_losses = []\n",
    "final_val_losses = []\n",
    "final_val_macro_f1s = []\n",
    "learning_rates = []\n",
    "\n",
    "for epoch in range(25):\n",
    "    # Training with Mixed Precision\n",
    "    final_model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/25\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = final_model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    final_train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation with Macro-F1\n",
    "    val_metrics = calculate_metrics(final_model, val_loader, device, criterion)\n",
    "    final_val_losses.append(val_metrics['loss'])\n",
    "    final_val_macro_f1s.append(val_metrics['macro_f1'])\n",
    "    \n",
    "    # Learning rate scheduling based on Macro-F1\n",
    "    scheduler.step(val_metrics['macro_f1'])\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train={avg_train_loss:.4f} | Val={val_metrics['loss']:.4f} | Macro-F1={val_metrics['macro_f1']:.4f} | LR={current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping check based on Macro-F1\n",
    "    if val_metrics['macro_f1'] > best_macro_f1:\n",
    "        best_macro_f1 = val_metrics['macro_f1']\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(final_model.state_dict(), 'experiments/final_model_best.pth')\n",
    "        print(f\"  ‚úì New best model saved (macro_f1={best_macro_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  ‚ö† No improvement ({patience_counter}/{patience})\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n‚õî Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Save final training results\n",
    "final_results = {\n",
    "    'architecture': best_arch_name,\n",
    "    'learning_rate': best_lr,\n",
    "    'epochs_trained': len(final_train_losses),\n",
    "    'train_losses': final_train_losses,\n",
    "    'val_losses': final_val_losses,\n",
    "    'val_macro_f1s': final_val_macro_f1s,\n",
    "    'learning_rates': learning_rates,\n",
    "    'best_macro_f1': best_macro_f1\n",
    "}\n",
    "\n",
    "with open('experiments/week3_final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "torch.save(final_model.state_dict(), 'experiments/final_model_last_epoch.pth')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Macro-F1: {best_macro_f1:.4f}\")\n",
    "print(f\"Total Epochs: {len(final_train_losses)}\")\n",
    "print(f\"Final Learning Rate: {learning_rates[-1]:.2e}\")\n",
    "print(f\"\\n‚úÖ Models saved:\")\n",
    "print(f\"   - experiments/final_model_best.pth (best macro-f1)\")\n",
    "print(f\"   - experiments/final_model_last_epoch.pth (last epoch)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5583e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final training results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "epochs_range = range(1, len(final_train_losses) + 1)\n",
    "\n",
    "# Training vs Validation Loss\n",
    "ax1.plot(epochs_range, final_train_losses, label='Train Loss', marker='o', linewidth=2)\n",
    "ax1.plot(epochs_range, final_val_losses, label='Val Loss', marker='s', linewidth=2)\n",
    "ax1.set_title('Final Training: Loss Curves', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Macro-F1 Curve\n",
    "ax2.plot(epochs_range, final_val_macro_f1s, label='Val Macro-F1', marker='o', linewidth=2, color='green')\n",
    "ax2.axhline(y=best_macro_f1, color='r', linestyle='--', label=f'Best={best_macro_f1:.4f}')\n",
    "ax2.set_title('Final Training: Macro-F1', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Macro-F1')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate Schedule\n",
    "ax2.plot(epochs_range, learning_rates, color='orange', marker='D', linewidth=2)\n",
    "ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Learning Rate')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting Gap (Train - Val Loss)\n",
    "gap = [t - v for t, v in zip(final_train_losses, final_val_losses)]\n",
    "ax3.plot(epochs_range, gap, color='purple', marker='^', linewidth=2)\n",
    "ax3.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax3.set_title('Overfitting Gap (Train Loss - Val Loss)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Gap')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary comparison: Week 1 ‚Üí Week 2 ‚Üí Week 3\n",
    "stages = ['Week 1\\n(Arch Search)', 'Week 2\\n(LR Tuning)', 'Week 3\\n(Final)']\n",
    "val_losses_summary = [\n",
    "    week1_summary['best_val_loss'],\n",
    "    week2_summary['best_val_loss'],\n",
    "    best_val_loss\n",
    "]\n",
    "\n",
    "bars = ax4.bar(stages, val_losses_summary, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.7)\n",
    "ax4.set_title('Progressive Improvement Across 3 Weeks', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Best Validation Loss')\n",
    "ax4.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, val_losses_summary):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiments/week3_final_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Final training visualizations saved to experiments/week3_final_training.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7755d37",
   "metadata": {},
   "source": [
    "## Summary: 3-Week Training Results\n",
    "\n",
    "**This notebook implements a systematic 3-week training approach:**\n",
    "\n",
    "### Week 1: Architecture Search (3-4 hours)\n",
    "- Tested 4 architecture variants (Light, Medium, Deep, MaxPool)\n",
    "- 3 epochs per architecture (fast screening)\n",
    "- Identified best architecture based on validation loss\n",
    "\n",
    "### Week 2: Hyperparameter Tuning (9-12 hours)\n",
    "- Tested 3 learning rates on best architecture\n",
    "- 6 epochs per configuration\n",
    "- Found optimal learning rate\n",
    "\n",
    "### Week 3: Final Training (10-12 hours)\n",
    "- Trained best model with optimal hyperparameters\n",
    "- Up to 25 epochs with early stopping\n",
    "- Learning rate scheduling and model checkpointing\n",
    "\n",
    "### Key Features:\n",
    "- **Fast**: Optimized for Kaggle T4 GPU (batch_size=4, 16GB VRAM)\n",
    "- **Reproducible**: All configurations saved to JSON\n",
    "- **Memory-efficient**: Sequential training, no grid search\n",
    "- **Well-documented**: Visualizations at each stage\n",
    "\n",
    "### Total GPU Time: ~25-32 hours ‚úÖ (fits within Kaggle's 30 hrs/week limit!)\n",
    "\n",
    "### ‚ö†Ô∏è Important: Train/Validation/Test Split\n",
    "\n",
    "**Proper Data Splitting:**\n",
    "- **Training set (80%)**: Used for model training (Weeks 1-3)\n",
    "- **Validation set (20% of train)**: Used for hyperparameter tuning\n",
    "- **Test set (completely held out)**: Only used ONCE for final evaluation after Week 3\n",
    "\n",
    "**Critical Rules:**\n",
    "1. ‚úÖ Normalization stats calculated **only from training data**\n",
    "2. ‚úÖ Validation set split from training data (80/20)\n",
    "3. ‚úÖ Test set never touched during Weeks 1-3\n",
    "4. ‚úÖ Final model evaluated on test set once after all tuning complete\n",
    "\n",
    "This prevents **data leakage** and ensures fair evaluation!\n",
    "\n",
    "**Next Steps**: Model evaluation and comparison with Random Forest baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1767f",
   "metadata": {},
   "source": [
    "## Final Evaluation on Test Set\n",
    "\n",
    "**Run this ONLY AFTER completing all 3 weeks of training!**\n",
    "\n",
    "This evaluates the final model on the completely held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model from Week 3\n",
    "final_model = arch_map[best_arch_name](\n",
    "    in_channels=len(dynamic_features)+4,\n",
    "    num_classes=5\n",
    ").to(device)\n",
    "\n",
    "final_model.load_state_dict(torch.load('experiments/final_model_best.pth'))\n",
    "final_model.eval()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test samples: {len(test_dates)}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_ds = AvalancheDataset(test_dates, dynamic_dir, target_dir, static_file, stats, dynamic_features)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        outputs = final_model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Get predictions (class with highest probability)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_predictions.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TEST SET RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"\\n‚ö†Ô∏è  This is the ONLY time the test set has been evaluated!\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save test results\n",
    "test_results = {\n",
    "    'test_loss': avg_test_loss,\n",
    "    'architecture': best_arch_name,\n",
    "    'learning_rate': best_lr,\n",
    "    'num_test_samples': len(test_dates)\n",
    "}\n",
    "\n",
    "with open('experiments/final_test_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Test results saved to experiments/final_test_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avalanche_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
