{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc79b42e",
   "metadata": {},
   "source": [
    "# **Avalanche Risk Project**\n",
    "\n",
    "Advanced Data Analytics, Fall 2025\n",
    "\n",
    "The following project examines the feasibility of deep learning models (3D-CNN, convLSTM) compared to traditional, state-of-the-art methods for predicting avalanche danger levels. Using a comprehensive dataset from the Swiss Federal Institute for Snow and Avalanche Research (SLF) spanning from 1997-2020, the project heavily relies on the work of PÃ©rez-GuillÃ©n et al. (2022) and Maissen et al. (2024) and adopts many of their preprocessing and modeling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Kaggle\n",
    "is_kaggle = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "if is_kaggle:\n",
    "    # Kaggle paths\n",
    "    CLEANED_DATA_PATH = '/kaggle/input/avalanche-data/data/cleaned_data.parquet'\n",
    "    grid_dir = '/kaggle/input/avalanche-data/data/grids'\n",
    "    \n",
    "    # Kaggle Output Paths (Writable)\n",
    "    output_dir = '/kaggle/working'\n",
    "    model_save_dir = os.path.join(output_dir, 'models')\n",
    "    \n",
    "    # Install necessary packages\n",
    "    print(\"Running on Kaggle - Installing packages...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', '-q', 'xarray-spatial', 'rasterio', 'geopandas'], check=True)\n",
    "else:\n",
    "    # Local Paths\n",
    "    CLEANED_DATA_PATH = '../data/cleaned_data.parquet'\n",
    "    grid_dir = '../data/grids'\n",
    "    \n",
    "    output_dir = 'results'\n",
    "    model_save_dir = 'models'\n",
    "    \n",
    "    print(\"Running Locally. Standard paths used.\")\n",
    "\n",
    "# Define grid subdirectories\n",
    "dynamic_dir = os.path.join(grid_dir, 'dynamic')\n",
    "target_dir = os.path.join(grid_dir, 'targets')\n",
    "static_file = os.path.join(grid_dir, 'static_terrain.npy')\n",
    "\n",
    "# Create output directories if they don't exist (critical for Kaggle)\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {CLEANED_DATA_PATH}\")\n",
    "print(f\"Grid directory: {grid_dir}\")\n",
    "print(f\"Model save directory: {model_save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b76784",
   "metadata": {},
   "source": [
    "## **5. CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a61502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURATION & FEATURE DEFINITION ---\n",
    "dynamic_features = [\n",
    "    'delta_elevation', 'Pen_depth', 'HN24', \n",
    "    'TA', 'wind_trans24', 'RH', \n",
    "    'min_ccl_pen', 'relative_load_3d', 'HS_mod',\n",
    "    'wind_u', 'wind_v' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_parquet(CLEANED_DATA_PATH)\n",
    "\n",
    "# Re-calculate Wind Vectors (if missing)\n",
    "if 'DW' in df.columns and 'VW' in df.columns:\n",
    "    wd_rad = np.deg2rad(df['DW'])\n",
    "    df['wind_u'] = -df['VW'] * np.sin(wd_rad)\n",
    "    df['wind_v'] = -df['VW'] * np.cos(wd_rad)\n",
    "    print(\"Wind vectors calculated.\")\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"Columns available: {df.columns.tolist()[:10]}...\")  # Show first 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a717339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3124\n",
      "  Date range: 1997-11-12 to 2016-05-08\n",
      "\n",
      "Validation samples (2016/17 + 2017/18 winters): 336\n",
      "  Date range: 2016-11-09 to 2018-04-14\n",
      "\n",
      "Test samples (held out): 359\n",
      "  Date range: 2019-11-16 to 2019-04-24\n",
      "\n",
      "âœ… Temporal split verified: train â†’ val (2 winters) â†’ test\n"
     ]
    }
   ],
   "source": [
    "# --- FIX 1: Proper Data Splitting (Train/Val/Test) ---\n",
    "# 1. Isolate Test Set (Don't touch until the very end!)\n",
    "test_dates = df[df['set'] == 'test']['datum'].unique().astype(str)\n",
    "\n",
    "# 2. Get all Training Dates\n",
    "all_train_dates = df[df['set'] == 'train']['datum'].unique()\n",
    "all_train_dates = pd.to_datetime(np.sort(all_train_dates))\n",
    "\n",
    "# 3. Split Training into Train (85%) and Validation (15%)\n",
    "# We split by time to respect temporal order\n",
    "split_idx = int(len(all_train_dates) * 0.85)\n",
    "train_dates = all_train_dates[:split_idx].astype(str)\n",
    "val_dates = all_train_dates[split_idx:].astype(str)\n",
    "\n",
    "print(f\"Train Days: {len(train_dates)} | Val Days: {len(val_dates)} | Test Days: {len(test_dates)}\")\n",
    "print(f\"  Train range: {train_dates[0]} to {train_dates[-1]}\")\n",
    "print(f\"  Val range: {val_dates[0]} to {val_dates[-1]}\")\n",
    "print(f\"  Test range: {test_dates[0]} to {test_dates[-1]}\")\n",
    "\n",
    "# Verify no temporal overlap\n",
    "assert pd.to_datetime(val_dates[0]) > pd.to_datetime(train_dates[-1]), \"Train-Val leakage!\"\n",
    "assert pd.to_datetime(test_dates[0]) > pd.to_datetime(val_dates[-1]), \"Val-Test leakage!\"\n",
    "print(\"\\nâœ… Temporal split verified: train â†’ val â†’ test (no leakage)\")\n",
    "\n",
    "# --- FIX 2: No-Leakage Normalization ---\n",
    "# Calculate Mean/Std ONLY on the training portion\n",
    "print(\"\\nCalculating normalization stats on Training set only...\")\n",
    "train_df_subset = df[df['datum'].astype(str).isin(train_dates)]\n",
    "\n",
    "stats = {col: {\n",
    "    'mean': train_df_subset[col].mean(), \n",
    "    'std': train_df_subset[col].std()\n",
    "} for col in dynamic_features}\n",
    "\n",
    "print(\"âœ… Normalization stats ready (calculated from training data only).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef1987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters - Optimized for Kaggle T4 GPU (16GB VRAM)\n",
    "batch_size = 4        # Kaggle T4 can handle 4-8, start with 4\n",
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "lookback = 7          # T-6 to T\n",
    "num_classes = 5       # 0=NoData (Ignore), 1, 2, 3, 4 (High)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Running on device: {device}\")\n",
    "print(f\"Batch size: {batch_size} (4x faster than batch_size=1)\")\n",
    "print(f\"Grid paths configured:\")\n",
    "print(f\"  Dynamic: {dynamic_dir}\")\n",
    "print(f\"  Target: {target_dir}\")\n",
    "print(f\"  Static: {static_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvalancheDataset(Dataset):\n",
    "    def __init__(self, date_list, feature_dir, target_dir, static_file, stats, features, lookback=7):\n",
    "        self.dates = date_list\n",
    "        self.feature_dir = feature_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.stats = stats\n",
    "        self.features = features\n",
    "        self.lookback = lookback\n",
    "        \n",
    "        # Load static data once (H, W, 4) -> (Elevation, Slope, AspectSin, AspectCos)\n",
    "        self.static_data = np.load(static_file).astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_date = pd.to_datetime(self.dates[idx])\n",
    "        \n",
    "        # 1. Build Dynamic Sequence (T-6 to T) \n",
    "        frames = []\n",
    "        start_date = target_date - pd.Timedelta(days=self.lookback - 1)\n",
    "        \n",
    "        for i in range(self.lookback):\n",
    "            d_str = (start_date + pd.Timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "            f_path = os.path.join(self.feature_dir, f\"{d_str}.npz\")\n",
    "            \n",
    "            if os.path.exists(f_path):\n",
    "                # Load raw data (H, W, C)\n",
    "                raw = np.load(f_path)['data']\n",
    "                norm = np.zeros_like(raw)\n",
    "                # Normalize channel-wise\n",
    "                for c, name in enumerate(self.features):\n",
    "                    mu, std = self.stats[name]['mean'], self.stats[name]['std']\n",
    "                    norm[..., c] = (raw[..., c] - mu) / (std + 1e-6)\n",
    "                frames.append(norm)\n",
    "            else:\n",
    "                # Missing day? Fill with zeros\n",
    "                frames.append(np.zeros_like(self.static_data[..., :len(self.features)]))\n",
    "        \n",
    "        # Stack -> (Time, H, W, C_dyn)\n",
    "        dynamic_tensor = np.stack(frames, axis=0)\n",
    "\n",
    "        # 2. Add Static Data \n",
    "        static_expanded = np.tile(self.static_data[np.newaxis, ...], (self.lookback, 1, 1, 1))\n",
    "        \n",
    "        # Combine -> (Time, H, W, Total_Channels)\n",
    "        full_cube = np.concatenate([dynamic_tensor, static_expanded], axis=-1)\n",
    "\n",
    "        # 3. Load Target \n",
    "        t_path = os.path.join(self.target_dir, f\"{self.dates[idx]}.npy\")\n",
    "        if os.path.exists(t_path):\n",
    "            label = np.load(t_path).astype(np.int64)\n",
    "            label[label == -1] = 0 # Move \"No Data\" to class 0\n",
    "        else:\n",
    "            label = np.zeros((self.static_data.shape[0], self.static_data.shape[1]), dtype=np.int64)\n",
    "\n",
    "        # Permute\n",
    "        X = torch.from_numpy(full_cube).permute(3, 0, 1, 2)\n",
    "        Y = torch.from_numpy(label)\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvalancheNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AvalancheNet, self).__init__()\n",
    "        \n",
    "        # 3D Convolutions: Process (Time, Height, Width)\n",
    "        # padding='same' keeps the output map size equal to input size\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        # Collapse Time Dimension: Average over the 7 days\n",
    "        # Output becomes (Batch, 64, H, W)\n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        \n",
    "        # Final 2D Convolutions to map to classes\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, C, T, H, W)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Aggregate Time: (Batch, 64, 1, H, W) -> Squeeze -> (Batch, 64, H, W)\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        \n",
    "        # Predict Classes\n",
    "        x = self.final_conv(x) # (Batch, Num_Classes, H, W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b93399",
   "metadata": {},
   "source": [
    "## Architecture Variants for Hyperparameter Tuning\n",
    "\n",
    "We'll test 4 different architectures to find the optimal model complexity:\n",
    "1. **Light**: Baseline 2-layer model (32â†’64 filters)\n",
    "2. **Medium**: Current model with dropout regularization\n",
    "3. **Deep**: 3-layer model (64â†’128â†’128 filters)\n",
    "4. **MaxPool**: Uses max pooling instead of average pooling for temporal aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f658562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvalancheNet_Light(nn.Module):\n",
    "    \"\"\"Lightweight baseline - 2 conv layers, 32â†’64 filters\"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AvalancheNet_Light, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AvalancheNet_Medium(nn.Module):\n",
    "    \"\"\"Medium model with dropout regularization - 32â†’64 filters + dropout\"\"\"\n",
    "    def __init__(self, in_channels, num_classes, dropout_rate=0.3):\n",
    "        super(AvalancheNet_Medium, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout3d(p=dropout_rate)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        self.dropout2 = nn.Dropout3d(p=dropout_rate)\n",
    "        \n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AvalancheNet_Deep(nn.Module):\n",
    "    \"\"\"Deeper model - 3 conv layers, 64â†’128â†’128 filters\"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AvalancheNet_Deep, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(128, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn3 = nn.BatchNorm3d(128)\n",
    "        \n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        self.final_conv = nn.Conv2d(128, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AvalancheNet_MaxPool(nn.Module):\n",
    "    \"\"\"Uses MaxPooling instead of AvgPooling - captures extreme events\"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AvalancheNet_MaxPool, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        # Key difference: MaxPooling captures peak conditions over 7 days\n",
    "        self.pool_time = nn.MaxPool3d((7, 1, 1))\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28119c",
   "metadata": {},
   "source": [
    "## Advanced Architectures\n",
    "\n",
    "These models address deeper spatial patterns and temporal sequence modeling:\n",
    "1. **DeepAvalancheNet**: 4-layer deep CNN with downsampling and upsampling for better spatial feature extraction\n",
    "2. **AvalancheConvLSTM**: ConvLSTM for capturing temporal evolution (e.g., storm â†’ settling transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAvalancheNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved 3D CNN: Deeper (4 layers) to capture larger spatial patterns.\n",
    "    Includes Dropout for regularization and spatial downsampling/upsampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(DeepAvalancheNet, self).__init__()\n",
    "        \n",
    "        # Block 1: Low-level features (e.g., local snow/wind gradients)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout3d(0.2)\n",
    "        )\n",
    "        \n",
    "        # Block 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),  # Downsample H/W only, keep Time\n",
    "            nn.Dropout3d(0.2)\n",
    "        )\n",
    "        \n",
    "        # Block 3: Mid-level features (e.g., regional patterns)\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout3d(0.3)\n",
    "        )\n",
    "        \n",
    "        # Block 4: High-level features\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)), \n",
    "            nn.Dropout3d(0.3)\n",
    "        )\n",
    "        \n",
    "        # Aggregate Time: Average over the 7-day window\n",
    "        self.pool_time = nn.AdaptiveAvgPool3d((1, None, None))\n",
    "        \n",
    "        # Upsample back to original map size (Segmentation logic)\n",
    "        self.upsample = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Final Classification\n",
    "        self.final_conv = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, C, T, H, W)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Collapse Time: (B, 256, 1, H/4, W/4) -> (B, 256, H/4, W/4)\n",
    "        x = self.pool_time(x).squeeze(2)\n",
    "        \n",
    "        # Restore Resolution: (B, 256, H, W)\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # Classify\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AvalancheConvLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvLSTM: Captures temporal sequence evolution (e.g., Storm -> Settling).\n",
    "    Better for 'Transition Days' where temporal dynamics matter.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_classes, hidden_dim=64):\n",
    "        super(AvalancheConvLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Feature Extractor (Spatial)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # LSTM Cell (Temporal)\n",
    "        # Simplified implementation: treating Time as Batch dim for Conv, then looping\n",
    "        self.lstm_conv = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.final = nn.Conv2d(hidden_dim, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, C, T, H, W)\n",
    "        b, c, t, h, w = x.size()\n",
    "        \n",
    "        # 1. Fold Time into Batch to process spatial features in parallel\n",
    "        # (B*T, C, H, W)\n",
    "        x_time_flat = x.permute(0, 2, 1, 3, 4).contiguous().view(b * t, c, h, w)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.conv(x_time_flat)\n",
    "        \n",
    "        # Unfold Time: (B, T, Hidden, H, W)\n",
    "        features = features.view(b, t, self.hidden_dim, h, w)\n",
    "        \n",
    "        # 2. Manual ConvLSTM Step (Last Day Prediction)\n",
    "        # Initialize hidden state (h) and cell state (c)\n",
    "        h = torch.zeros(b, self.hidden_dim, h, w).to(x.device)\n",
    "        c = torch.zeros(b, self.hidden_dim, h, w).to(x.device)\n",
    "        \n",
    "        # Iterate through time steps\n",
    "        for i in range(t):\n",
    "            xt = features[:, i, :, :, :]\n",
    "            combined = torch.cat([xt, h], dim=1)\n",
    "            \n",
    "            # Gates (Input, Forget, Output, Cell)\n",
    "            gates = self.lstm_conv(combined)\n",
    "            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "            \n",
    "            ingate = torch.sigmoid(ingate)\n",
    "            forgetgate = torch.sigmoid(forgetgate)\n",
    "            cellgate = torch.tanh(cellgate)\n",
    "            outgate = torch.sigmoid(outgate)\n",
    "            \n",
    "            c = (forgetgate * c) + (ingate * cellgate)\n",
    "            h = outgate * torch.tanh(c)\n",
    "            \n",
    "        # 3. Final Prediction based on last hidden state\n",
    "        out = self.final(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e53a30",
   "metadata": {},
   "source": [
    "### How to Use Advanced Models\n",
    "\n",
    "You can easily switch between architectures in the model initialization cell:\n",
    "\n",
    "````python\n",
    "# Option 1: Deep 3D CNN (Recommended for spatial patterns)\n",
    "model = DeepAvalancheNet(in_channels=len(dynamic_features)+4, num_classes=5).to(device)\n",
    "\n",
    "# Option 2: ConvLSTM (Best for temporal transitions)\n",
    "model = AvalancheConvLSTM(in_channels=len(dynamic_features)+4, num_classes=5).to(device)\n",
    "````\n",
    "\n",
    "**DeepAvalancheNet** benefits:\n",
    "- 4 convolutional layers capture hierarchical spatial patterns\n",
    "- Downsampling (2x2) increases receptive field\n",
    "- Upsampling restores full resolution for pixel-wise classification\n",
    "- Better for capturing regional avalanche patterns\n",
    "\n",
    "**AvalancheConvLSTM** benefits:\n",
    "- Models temporal evolution across the 7-day sequence\n",
    "- Captures storm â†’ settling transitions\n",
    "- Better for days with rapid weather changes\n",
    "- Each time step influences the next (not just aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fedafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, dataloader, device, criterion):\n",
    "    \"\"\"\n",
    "    Calculate validation metrics including Macro-F1 score.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains loss, macro_f1, and per-class F1 scores\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions (argmax over class dimension)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Flatten spatial dimensions and filter out ignore_index (0)\n",
    "            preds_flat = preds.cpu().numpy().flatten()\n",
    "            targets_flat = y.cpu().numpy().flatten()\n",
    "            \n",
    "            # Only keep non-zero labels (ignore background)\n",
    "            mask = targets_flat != 0\n",
    "            preds_flat = preds_flat[mask]\n",
    "            targets_flat = targets_flat[mask]\n",
    "            \n",
    "            all_preds.extend(preds_flat)\n",
    "            all_targets.extend(targets_flat)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Calculate Macro-F1 (average F1 across classes 1-4)\n",
    "    macro_f1 = f1_score(all_targets, all_preds, average='macro', labels=[1, 2, 3, 4], zero_division=0)\n",
    "    \n",
    "    # Per-class F1 scores\n",
    "    per_class_f1 = f1_score(all_targets, all_preds, average=None, labels=[1, 2, 3, 4], zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'macro_f1': macro_f1,\n",
    "        'f1_class_1': per_class_f1[0],\n",
    "        'f1_class_2': per_class_f1[1],\n",
    "        'f1_class_3': per_class_f1[2],\n",
    "        'f1_class_4': per_class_f1[3]\n",
    "    }"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c2d1651f",
   "metadata": {},
   "source": [
    "Now, from the EDA we know that we are dealing with a highly imbalanced dataset. To tackle this, we will implement a weighted cross-entropy loss function that assigns higher weights to the minority classes. This approach will help the model pay more attention to underrepresented classes during training. We create training and test sets and use ADAM optimizer for training the CNN model.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31584074",
   "metadata": {},
   "source": [
    "### Model Architecture Summary\n",
    "\n",
    "**Input Tensor Shape**: `(Batch=4, Channels=15, Time=7, Height=211, Width=470)`\n",
    "- **11 dynamic features** (weather/snow conditions over 7 days)\n",
    "- **4 static features** (terrain: elevation, slope, aspect_sin, aspect_cos)\n",
    "\n",
    "**3D CNN Processing**:\n",
    "1. Conv3D layer 1: Extracts spatio-temporal patterns â†’ 32 filters\n",
    "2. Conv3D layer 2: Deeper features â†’ 64 filters  \n",
    "3. Temporal pooling: Aggregates 7 days into single time step\n",
    "4. Conv2D output: Maps to 5 danger level classes per pixel\n",
    "\n",
    "**Output**: `(Batch=4, Classes=5, Height=211, Width=470)` - Danger level prediction for each grid cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb6bc8",
   "metadata": {},
   "source": [
    "## 2-3 Week Training Plan\n",
    "\n",
    "**Week 1: Architecture Search (4 runs Ã— 5 epochs â‰ˆ 8-12 hours)**\n",
    "- Test Light, Medium, Deep, MaxPool variants\n",
    "- Identify best architecture based on validation loss\n",
    "\n",
    "**Week 2: Hyperparameter Tuning (6 runs Ã— 10 epochs â‰ˆ 20-25 hours)**\n",
    "- Test 3 learning rates on best architecture\n",
    "- Test 3 dropout rates for regularization\n",
    "\n",
    "**Week 3: Final Training (2 runs Ã— 20 epochs â‰ˆ 12-16 hours)**\n",
    "- Train best model with optimal hyperparameters\n",
    "- Full 20-30 epoch training for final evaluation\n",
    "\n",
    "**Total: ~40-50 GPU hours on Kaggle (well within 30 hours/week limit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761a529",
   "metadata": {},
   "source": [
    "## âš¡ Speed Optimization for Kaggle\n",
    "\n",
    "**Updated for Kaggle T4 GPU (16GB VRAM):**\n",
    "- **Batch size**: Increased from 1 â†’ 4 (**4x faster!**)\n",
    "- **Epochs reduced** for initial screening (3 for Week 1, 6 for Week 2)\n",
    "- **Week 3**: Still 25 epochs for final model\n",
    "\n",
    "### Realistic Timeline on Kaggle:\n",
    "\n",
    "| Week | Task | GPU Hours | Speed |\n",
    "|------|------|-----------|-------|\n",
    "| **Week 1** | 4 archs Ã— 3 epochs | **~6-8 hours** | 4x faster |\n",
    "| **Week 2** | 3 LRs Ã— 6 epochs | **~9-12 hours** | 4x faster |\n",
    "| **Week 3** | Final Ã— 20 epochs | **~10-12 hours** | 4x faster |\n",
    "| **TOTAL** | | **~25-32 hours** âœ… | Fits in limit! |\n",
    "\n",
    "**Memory Check**: T4 has 16GB VRAM, your tensors at batch_size=4:\n",
    "- Input: `(4, 15, 7, 211, 470) Ã— 4 bytes = 1.5 GB`\n",
    "- Activations: ~4-6 GB\n",
    "- **Total: ~8-10 GB** âœ… (well under 16GB limit)\n",
    "\n",
    "### ðŸš¨ Important: Validation vs Test\n",
    "\n",
    "**Currently, this notebook uses VALIDATION (test set from your data split):**\n",
    "```python\n",
    "val_dates = df[df['set'] == 'test']['datum'].unique()\n",
    "```\n",
    "\n",
    "**This is correct for hyperparameter tuning!** You should:\n",
    "1. âœ… Tune on validation set (current approach)\n",
    "2. âŒ Never touch true test set until final evaluation\n",
    "3. ðŸŽ¯ After Week 3, evaluate final model on held-out test set once\n",
    "\n",
    "Your data has `set` column with values like `'train'` and `'test'`. The `'test'` portion is actually your **validation set** for tuning. You'll need a separate true test set for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cec5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Load dates and calculate stats (fast)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_meta = \u001b[43mpd\u001b[49m.read_parquet(\u001b[33m'\u001b[39m\u001b[33m../data/cleaned_data.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m train_dates = df_meta[df_meta[\u001b[33m'\u001b[39m\u001b[33mset\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdatum\u001b[39m\u001b[33m'\u001b[39m].unique().astype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m      4\u001b[39m val_dates = df_meta[df_meta[\u001b[33m'\u001b[39m\u001b[33mset\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdatum\u001b[39m\u001b[33m'\u001b[39m].unique().astype(\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# --- FIX 3: Class Weights for Imbalance ---\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 1. Print class distribution\n",
    "if 'danger_level' in train_df_subset.columns:\n",
    "    freq_dangerLevel = train_df_subset['danger_level'].value_counts(normalize=True).sort_index()\n",
    "    print(f\"Class Distribution in Training Data:\\n{freq_dangerLevel}\\n\")\n",
    "    \n",
    "    # Calculate weights based on training labels\n",
    "    y_train_labels = train_df_subset['danger_level'].values.astype(int)\n",
    "    classes = np.unique(y_train_labels)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_labels)\n",
    "    \n",
    "    # Convert to Tensor (Index 0 is 'No Data', mapped to weight 0)\n",
    "    # Classes in data are 1, 2, 3, 4. Indices in tensor are 0, 1, 2, 3, 4.\n",
    "    final_weights = torch.tensor([0.0] + list(weights), dtype=torch.float32)\n",
    "    \n",
    "    print(f\"Class Weights (balanced): {final_weights}\")\n",
    "else:\n",
    "    # Fallback: Use inverse frequency from original distribution\n",
    "    class_counts = torch.tensor([\n",
    "        0.0,      # Class 0 (NoData) - will be ignored anyway\n",
    "        0.211133, # Class 1\n",
    "        0.413133, # Class 2\n",
    "        0.358004, # Class 3\n",
    "        0.017729  # Class 4 \n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    final_weights = torch.zeros(5, dtype=torch.float32)\n",
    "    final_weights[1:] = 1.0 / class_counts[1:]\n",
    "    final_weights = final_weights / final_weights[1:].sum()\n",
    "    print(f\"Class Weights (inverse frequency): {final_weights}\")\n",
    "\n",
    "# 2. Instantiate Datasets\n",
    "train_ds = AvalancheDataset(train_dates, dynamic_dir, target_dir, static_file, stats, dynamic_features)\n",
    "val_ds = AvalancheDataset(val_dates, dynamic_dir, target_dir, static_file, stats, dynamic_features)\n",
    "\n",
    "# Use num_workers for faster data loading (Kaggle optimization)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "# 3. Initialize Model\n",
    "# Input channels = 11 dynamic + 4 static = 15\n",
    "model = AvalancheNet(in_channels=len(dynamic_features)+4, num_classes=5).to(device)\n",
    "\n",
    "# 4. Loss Function with Class Weights\n",
    "criterion = nn.CrossEntropyLoss(weight=final_weights.to(device), ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Add Scheduler (Optional but recommended)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5)\n",
    "\n",
    "print(\"\\nâœ… Model, DataLoaders, and Weighted Loss Function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e3c30",
   "metadata": {},
   "source": [
    "## WEEK 1: Architecture Search\n",
    "\n",
    "Run this cell to compare all 4 architectures with 5 epochs each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe9ff2",
   "metadata": {},
   "source": [
    "## Robust Training Loop with Checkpoints\n",
    "\n",
    "This training loop includes:\n",
    "- **Checkpoint saving/resuming**: Prevents data loss from crashes\n",
    "- **Macro-F1 tracking**: Primary metric for imbalanced data\n",
    "- **Early stopping**: Stops when validation performance plateaus\n",
    "- **Mixed precision**: Faster training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06587293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Checkpoint Configuration\n",
    "CHECKPOINT_DIR = 'models/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, 'best_cnn_model.pth')\n",
    "LAST_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'last_checkpoint.pth')\n",
    "\n",
    "# Helper to save checkpoint\n",
    "def save_checkpoint(state, is_best, filename):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        torch.save(state, BEST_MODEL_PATH)\n",
    "        print(f\"  ðŸ”¥ New Best Model saved to {BEST_MODEL_PATH}\")\n",
    "\n",
    "# --- RESUME LOGIC ---\n",
    "start_epoch = 0\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "if os.path.exists(LAST_CHECKPOINT_PATH):\n",
    "    print(f\"ðŸ”„ Resuming from checkpoint: {LAST_CHECKPOINT_PATH}\")\n",
    "    checkpoint = torch.load(LAST_CHECKPOINT_PATH)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_f1 = checkpoint['best_val_f1']\n",
    "    print(f\"Resuming at Epoch {start_epoch} with Best F1: {best_val_f1:.4f}\")\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "scaler = GradScaler()  # For Mixed Precision (Speed)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epochs: {start_epoch} â†’ {epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # 1. TRAINING PHASE\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "    for X, Y in loop:\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Context\n",
    "        with autocast():\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, Y)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # 2. VALIDATION PHASE\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            outputs = model(X)\n",
    "            \n",
    "            # Get class predictions (Argmax)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            # Flatten and mask \"No Data\" (0) pixels for metric calc\n",
    "            mask = Y != 0\n",
    "            val_preds.append(preds[mask].cpu())\n",
    "            val_targets.append(Y[mask].cpu())\n",
    "            \n",
    "    # Concatenate all batches\n",
    "    val_preds = torch.cat(val_preds).numpy()\n",
    "    val_targets = torch.cat(val_targets).numpy()\n",
    "    \n",
    "    # Calculate Macro F1\n",
    "    val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Macro-F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Update Scheduler\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # 3. SAVE CHECKPOINT\n",
    "    is_best = val_f1 > best_val_f1\n",
    "    if is_best:\n",
    "        best_val_f1 = val_f1\n",
    "        print(f\"  ðŸ”¥ New Best Model! Macro-F1={best_val_f1:.4f}\")\n",
    "        \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, LAST_CHECKPOINT_PATH)\n",
    "    \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Validation Macro-F1: {best_val_f1:.4f}\")\n",
    "print(f\"Best model saved to: {BEST_MODEL_PATH}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e72669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create experiment tracking directory\n",
    "Path(\"experiments\").mkdir(exist_ok=True)\n",
    "\n",
    "# Define architectures to test\n",
    "architectures = {\n",
    "    'light': AvalancheNet_Light,\n",
    "    'medium': AvalancheNet_Medium,\n",
    "    'deep': AvalancheNet_Deep,\n",
    "    'maxpool': AvalancheNet_MaxPool,\n",
    "    'deep_cnn': DeepAvalancheNet,      # NEW: Deep 4-layer CNN\n",
    "    'convlstm': AvalancheConvLSTM      # NEW: ConvLSTM\n",
    "}\n",
    "\n",
    "# Configuration for architecture search (shorter epochs)\n",
    "search_config = {\n",
    "    'batch_size': 4,  # Kaggle T4: 16GB VRAM (4x faster than batch_size=1)\n",
    "    'learning_rate': 1e-4,\n",
    "    'epochs': 3,  # Reduced for faster comparison (was 5)\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ARCHITECTURE SEARCH - WEEK 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for arch_name, arch_class in architectures.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {arch_name.upper()}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = arch_class(\n",
    "        in_channels=len(dynamic_features)+4, \n",
    "        num_classes=5\n",
    "    ).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\\n\")\n",
    "    \n",
    "    # Setup optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=search_config['learning_rate'])\n",
    "    scaler = GradScaler()  # AMP for faster training\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_macro_f1s = []\n",
    "    \n",
    "    for epoch in range(search_config['epochs']):\n",
    "        # Training with Mixed Precision\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{search_config['epochs']}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "            \n",
    "            # Scaled backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation with Macro-F1\n",
    "        val_metrics = calculate_metrics(model, val_loader, device, criterion)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_macro_f1s.append(val_metrics['macro_f1'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f} | Val Loss={val_metrics['loss']:.4f} | Macro-F1={val_metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results[arch_name] = {\n",
    "        'total_params': total_params,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_macro_f1s': val_macro_f1s,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'final_macro_f1': val_macro_f1s[-1]\n",
    "    }\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    torch.save(\n",
    "        model.state_dict(), \n",
    "        f\"experiments/week1_{arch_name}_final.pth\"\n",
    "    )\n",
    "    \n",
    "    # Save config\n",
    "    with open(f\"experiments/week1_{arch_name}_config.json\", 'w') as f:\n",
    "        json.dump({\n",
    "            'architecture': arch_name,\n",
    "            'config': search_config,\n",
    "            'results': results[arch_name]\n",
    "        }, f, indent=2)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 1 RESULTS: ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for arch_name, metrics in results.items():\n",
    "    print(f\"\\n{arch_name.upper()}:\")\n",
    "    print(f\"  Parameters: {metrics['total_params']:,}\")\n",
    "    print(f\"  Final Train Loss: {metrics['final_train_loss']:.4f}\")\n",
    "    print(f\"  Final Val Loss: {metrics['final_val_loss']:.4f}\")\n",
    "    print(f\"  Final Macro-F1: {metrics['final_macro_f1']:.4f}\")\n",
    "\n",
    "# Find best architecture based on Macro-F1 (not loss!)\n",
    "best_arch = max(results.items(), key=lambda x: x[1]['final_macro_f1'])\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ† BEST ARCHITECTURE: {best_arch[0].upper()}\")\n",
    "print(f\"   Validation Loss: {best_arch[1]['final_val_loss']:.4f}\")\n",
    "print(f\"   Macro-F1: {best_arch[1]['final_macro_f1']:.4f}\")\n",
    "print(f\"   Parameters: {best_arch[1]['total_params']:,}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save summary\n",
    "with open('experiments/week1_summary.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'all_results': results,\n",
    "        'best_architecture': best_arch[0],\n",
    "        'best_val_loss': best_arch[1]['final_val_loss']\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training curves for all architectures\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Training Loss Comparison\n",
    "for arch_name, metrics in results.items():\n",
    "    epochs_range = range(1, len(metrics['train_losses']) + 1)\n",
    "    ax1.plot(epochs_range, metrics['train_losses'], label=arch_name, marker='o')\n",
    "\n",
    "ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss Comparison\n",
    "for arch_name, metrics in results.items():\n",
    "    epochs_range = range(1, len(metrics['val_losses']) + 1)\n",
    "    ax2.plot(epochs_range, metrics['val_losses'], label=arch_name, marker='o')\n",
    "\n",
    "ax2.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Macro-F1 Comparison\n",
    "for arch_name, metrics in results.items():\n",
    "    epochs_range = range(1, len(metrics['val_macro_f1s']) + 1)\n",
    "    ax3.plot(epochs_range, metrics['val_macro_f1s'], label=arch_name, marker='s', linewidth=2)\n",
    "\n",
    "ax3.set_title('Validation Macro-F1 Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Macro-F1')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter vs Performance scatter plot\n",
    "arch_names = list(results.keys())\n",
    "params = [results[a]['total_params'] / 1e6 for a in arch_names]  # In millions\n",
    "macro_f1s = [results[a]['final_macro_f1'] for a in arch_names]\n",
    "\n",
    "scatter = ax4.scatter(params, macro_f1s, s=300, alpha=0.6, c=range(len(arch_names)), cmap='viridis')\n",
    "for i, name in enumerate(arch_names):\n",
    "    ax4.annotate(name.upper(), (params[i], macro_f1s[i]), \n",
    "                xytext=(10, 5), textcoords='offset points', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax4.set_xlabel('Model Size (Million Parameters)', fontsize=12)\n",
    "ax4.set_ylabel('Final Macro-F1', fontsize=12)\n",
    "ax4.set_title('Model Complexity vs Macro-F1', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiments/week1_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Visualizations saved to experiments/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c779d9",
   "metadata": {},
   "source": [
    "## WEEK 2: Hyperparameter Tuning\n",
    "\n",
    "**Instructions**: \n",
    "1. Load the best architecture from Week 1 results\n",
    "2. Test different learning rates and dropout values\n",
    "3. Run this after reviewing Week 1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b35b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Week 1 results to determine best architecture\n",
    "with open('experiments/week1_summary.json', 'r') as f:\n",
    "    week1_summary = json.load(f)\n",
    "\n",
    "best_arch_name = week1_summary['best_architecture']\n",
    "print(f\"ðŸ“Œ Using best architecture from Week 1: {best_arch_name.upper()}\\n\")\n",
    "\n",
    "# Map architecture name to class\n",
    "arch_map = {\n",
    "    'light': AvalancheNet_Light,\n",
    "    'medium': AvalancheNet_Medium,\n",
    "    'deep': AvalancheNet_Deep,\n",
    "    'maxpool': AvalancheNet_MaxPool,\n",
    "    'deep_cnn': DeepAvalancheNet,      # NEW: Deep 4-layer CNN\n",
    "    'convlstm': AvalancheConvLSTM      # NEW: ConvLSTM\n",
    "}\n",
    "BestArchitecture = arch_map[best_arch_name]\n",
    "\n",
    "# Hyperparameter grid\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "dropout_rates = [0.2, 0.3, 0.5] if best_arch_name == 'medium' else [None]\n",
    "\n",
    "# Configuration for hyperparameter tuning\n",
    "tuning_config = {\n",
    "    'batch_size': 4,  # 4x faster than batch_size=1\n",
    "    'epochs': 6,  # Reduced from 10 (still enough to see trends)\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "week2_results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - WEEK 2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test learning rates\n",
    "for lr in learning_rates:\n",
    "    run_name = f\"lr_{lr:.0e}\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing Learning Rate: {lr}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    model = BestArchitecture(\n",
    "        in_channels=len(dynamic_features)+4,\n",
    "        num_classes=5\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_macro_f1s = []\n",
    "    \n",
    "    for epoch in range(tuning_config['epochs']):\n",
    "        # Training with AMP\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation with Macro-F1\n",
    "        val_metrics = calculate_metrics(model, val_loader, device, criterion)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_macro_f1s.append(val_metrics['macro_f1'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train={avg_train_loss:.4f} | Val={val_metrics['loss']:.4f} | Macro-F1={val_metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    week2_results[run_name] = {\n",
    "        'lr': lr,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_macro_f1s': val_macro_f1s,\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'final_macro_f1': val_macro_f1s[-1]\n",
    "    }\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"experiments/week2_{run_name}.pth\")\n",
    "    \n",
    "    with open(f\"experiments/week2_{run_name}_config.json\", 'w') as f:\n",
    "        json.dump({\n",
    "            'architecture': best_arch_name,\n",
    "            'learning_rate': lr,\n",
    "            'config': tuning_config,\n",
    "            'results': week2_results[run_name]\n",
    "        }, f, indent=2)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 2 RESULTS: LEARNING RATE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for run_name, metrics in week2_results.items():\n",
    "    print(f\"\\n{run_name}:\")\n",
    "    print(f\"  Learning Rate: {metrics['lr']}\")\n",
    "    print(f\"  Final Val Loss: {metrics['final_val_loss']:.4f}\")\n",
    "    print(f\"  Final Macro-F1: {metrics['final_macro_f1']:.4f}\")\n",
    "\n",
    "best_lr_run = max(week2_results.items(), key=lambda x: x[1]['final_macro_f1'])\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ† BEST LEARNING RATE: {best_lr_run[1]['lr']}\")\n",
    "print(f\"   Validation Loss: {best_lr_run[1]['final_val_loss']:.4f}\")\n",
    "print(f\"   Macro-F1: {best_lr_run[1]['final_macro_f1']:.4f}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save Week 2 summary\n",
    "with open('experiments/week2_summary.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'architecture': best_arch_name,\n",
    "        'all_results': week2_results,\n",
    "        'best_lr': best_lr_run[1]['lr'],\n",
    "        'best_val_loss': best_lr_run[1]['final_val_loss']\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd36075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Week 2 results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation Loss Comparison\n",
    "for run_name, metrics in week2_results.items():\n",
    "    epochs_range = range(1, len(metrics['val_losses']) + 1)\n",
    "    ax1.plot(epochs_range, metrics['val_losses'], \n",
    "            label=f\"LR={metrics['lr']:.0e}\", marker='o', linewidth=2)\n",
    "\n",
    "ax1.set_title('Learning Rate Comparison - Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Macro-F1 Comparison\n",
    "for run_name, metrics in week2_results.items():\n",
    "    epochs_range = range(1, len(metrics['val_macro_f1s']) + 1)\n",
    "    ax2.plot(epochs_range, metrics['val_macro_f1s'], \n",
    "            label=f\"LR={metrics['lr']:.0e}\", marker='s', linewidth=2)\n",
    "\n",
    "ax2.set_title('Learning Rate Comparison - Macro-F1', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Macro-F1', fontsize=12)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiments/week2_lr_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Week 2 visualization saved to experiments/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719ea5b",
   "metadata": {},
   "source": [
    "## WEEK 3: Final Training\n",
    "\n",
    "**Instructions**: \n",
    "1. Load best architecture and hyperparameters from Week 1-2\n",
    "2. Train for 20-30 epochs with early stopping\n",
    "3. Save final model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best configuration from Week 1 and Week 2\n",
    "with open('experiments/week1_summary.json', 'r') as f:\n",
    "    week1_summary = json.load(f)\n",
    "\n",
    "with open('experiments/week2_summary.json', 'r') as f:\n",
    "    week2_summary = json.load(f)\n",
    "\n",
    "best_arch_name = week1_summary['best_architecture']\n",
    "best_lr = week2_summary['best_lr']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL TRAINING - WEEK 3\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Architecture: {best_arch_name.upper()}\")\n",
    "print(f\"Learning Rate: {best_lr}\")\n",
    "print(f\"Epochs: 25 (with early stopping)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Initialize final model\n",
    "BestArchitecture = arch_map[best_arch_name]\n",
    "final_model = BestArchitecture(\n",
    "    in_channels=len(dynamic_features)+4,\n",
    "    num_classes=5\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_lr)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Learning rate scheduler (reduces LR when Macro-F1 plateaus)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Early stopping based on Macro-F1\n",
    "best_macro_f1 = 0.0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "final_train_losses = []\n",
    "final_val_losses = []\n",
    "final_val_macro_f1s = []\n",
    "learning_rates = []\n",
    "\n",
    "for epoch in range(25):\n",
    "    # Training with Mixed Precision\n",
    "    final_model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/25\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = final_model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    final_train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation with Macro-F1\n",
    "    val_metrics = calculate_metrics(final_model, val_loader, device, criterion)\n",
    "    final_val_losses.append(val_metrics['loss'])\n",
    "    final_val_macro_f1s.append(val_metrics['macro_f1'])\n",
    "    \n",
    "    # Learning rate scheduling based on Macro-F1\n",
    "    scheduler.step(val_metrics['macro_f1'])\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train={avg_train_loss:.4f} | Val={val_metrics['loss']:.4f} | Macro-F1={val_metrics['macro_f1']:.4f} | LR={current_lr:.2e}\")\n",
    "    \n",
    "    # Early stopping check based on Macro-F1\n",
    "    if val_metrics['macro_f1'] > best_macro_f1:\n",
    "        best_macro_f1 = val_metrics['macro_f1']\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(final_model.state_dict(), 'experiments/final_model_best.pth')\n",
    "        print(f\"  âœ“ New best model saved (macro_f1={best_macro_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  âš  No improvement ({patience_counter}/{patience})\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nâ›” Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Save final training results\n",
    "final_results = {\n",
    "    'architecture': best_arch_name,\n",
    "    'learning_rate': best_lr,\n",
    "    'epochs_trained': len(final_train_losses),\n",
    "    'train_losses': final_train_losses,\n",
    "    'val_losses': final_val_losses,\n",
    "    'val_macro_f1s': final_val_macro_f1s,\n",
    "    'learning_rates': learning_rates,\n",
    "    'best_macro_f1': best_macro_f1\n",
    "}\n",
    "\n",
    "with open('experiments/week3_final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "torch.save(final_model.state_dict(), 'experiments/final_model_last_epoch.pth')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Macro-F1: {best_macro_f1:.4f}\")\n",
    "print(f\"Total Epochs: {len(final_train_losses)}\")\n",
    "print(f\"Final Learning Rate: {learning_rates[-1]:.2e}\")\n",
    "print(f\"\\nâœ… Models saved:\")\n",
    "print(f\"   - experiments/final_model_best.pth (best macro-f1)\")\n",
    "print(f\"   - experiments/final_model_last_epoch.pth (last epoch)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5583e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final training results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "epochs_range = range(1, len(final_train_losses) + 1)\n",
    "\n",
    "# Training vs Validation Loss\n",
    "ax1.plot(epochs_range, final_train_losses, label='Train Loss', marker='o', linewidth=2)\n",
    "ax1.plot(epochs_range, final_val_losses, label='Val Loss', marker='s', linewidth=2)\n",
    "ax1.set_title('Final Training: Loss Curves', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Macro-F1 Curve\n",
    "ax2.plot(epochs_range, final_val_macro_f1s, label='Val Macro-F1', marker='o', linewidth=2, color='green')\n",
    "ax2.axhline(y=best_macro_f1, color='r', linestyle='--', label=f'Best={best_macro_f1:.4f}')\n",
    "ax2.set_title('Final Training: Macro-F1', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Macro-F1')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate Schedule\n",
    "ax2.plot(epochs_range, learning_rates, color='orange', marker='D', linewidth=2)\n",
    "ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Learning Rate')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting Gap (Train - Val Loss)\n",
    "gap = [t - v for t, v in zip(final_train_losses, final_val_losses)]\n",
    "ax3.plot(epochs_range, gap, color='purple', marker='^', linewidth=2)\n",
    "ax3.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax3.set_title('Overfitting Gap (Train Loss - Val Loss)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Gap')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Summary comparison: Week 1 â†’ Week 2 â†’ Week 3\n",
    "stages = ['Week 1\\n(Arch Search)', 'Week 2\\n(LR Tuning)', 'Week 3\\n(Final)']\n",
    "val_losses_summary = [\n",
    "    week1_summary['best_val_loss'],\n",
    "    week2_summary['best_val_loss'],\n",
    "    final_results['best_macro_f1']  # Use best_macro_f1 instead\n",
    "]\n",
    "\n",
    "bars = ax4.bar(stages, val_losses_summary, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.7)\n",
    "ax4.set_title('Progressive Improvement Across 3 Weeks', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Best Validation Loss')\n",
    "ax4.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, val_losses_summary):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('experiments/week3_final_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Final training visualizations saved to experiments/week3_final_training.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7755d37",
   "metadata": {},
   "source": [
    "## Summary: 3-Week Training Results\n",
    "\n",
    "**This notebook implements a systematic 3-week training approach:**\n",
    "\n",
    "### Week 1: Architecture Search (3-4 hours)\n",
    "- Tested 4 architecture variants (Light, Medium, Deep, MaxPool)\n",
    "- 3 epochs per architecture (fast screening)\n",
    "- Identified best architecture based on validation loss\n",
    "\n",
    "### Week 2: Hyperparameter Tuning (9-12 hours)\n",
    "- Tested 3 learning rates on best architecture\n",
    "- 6 epochs per configuration\n",
    "- Found optimal learning rate\n",
    "\n",
    "### Week 3: Final Training (10-12 hours)\n",
    "- Trained best model with optimal hyperparameters\n",
    "- Up to 25 epochs with early stopping\n",
    "- Learning rate scheduling and model checkpointing\n",
    "\n",
    "### Key Features:\n",
    "- **Fast**: Optimized for Kaggle T4 GPU (batch_size=4, 16GB VRAM)\n",
    "- **Reproducible**: All configurations saved to JSON\n",
    "- **Memory-efficient**: Sequential training, no grid search\n",
    "- **Well-documented**: Visualizations at each stage\n",
    "\n",
    "### Total GPU Time: ~25-32 hours âœ… (fits within Kaggle's 30 hrs/week limit!)\n",
    "\n",
    "### âš ï¸ Important: Train/Validation/Test Split\n",
    "\n",
    "**Proper Data Splitting:**\n",
    "- **Training set (80%)**: Used for model training (Weeks 1-3)\n",
    "- **Validation set (20% of train)**: Used for hyperparameter tuning\n",
    "- **Test set (completely held out)**: Only used ONCE for final evaluation after Week 3\n",
    "\n",
    "**Critical Rules:**\n",
    "1. âœ… Normalization stats calculated **only from training data**\n",
    "2. âœ… Validation set split from training data (80/20)\n",
    "3. âœ… Test set never touched during Weeks 1-3\n",
    "4. âœ… Final model evaluated on test set once after all tuning complete\n",
    "\n",
    "This prevents **data leakage** and ensures fair evaluation!\n",
    "\n",
    "**Next Steps**: Model evaluation and comparison with Random Forest baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1767f",
   "metadata": {},
   "source": [
    "## Final Evaluation on Test Set\n",
    "\n",
    "**Run this ONLY AFTER completing all 3 weeks of training!**\n",
    "\n",
    "This evaluates the final model on the completely held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model from Week 3\n",
    "final_model = arch_map[best_arch_name](\n",
    "    in_channels=len(dynamic_features)+4,\n",
    "    num_classes=5\n",
    ").to(device)\n",
    "\n",
    "final_model.load_state_dict(torch.load('experiments/final_model_best.pth'))\n",
    "final_model.eval()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test samples: {len(test_dates)}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_ds = AvalancheDataset(test_dates, dynamic_dir, target_dir, static_file, stats, dynamic_features)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_loader, desc=\"Testing\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        outputs = final_model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Get predictions (class with highest probability)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_predictions.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TEST SET RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"\\nâš ï¸  This is the ONLY time the test set has been evaluated!\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save test results\n",
    "test_results = {\n",
    "    'test_loss': avg_test_loss,\n",
    "    'architecture': best_arch_name,\n",
    "    'learning_rate': best_lr,\n",
    "    'num_test_samples': len(test_dates)\n",
    "}\n",
    "\n",
    "with open('experiments/final_test_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Test results saved to experiments/final_test_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avalanche_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
